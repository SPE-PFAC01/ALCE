{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9cc99dc",
   "metadata": {},
   "source": [
    "# Virtual Flow Meter Development for ESP Wells\n",
    "\n",
    "This notebook develops a **virtual flow meter** using surface measurements only, assuming **no downhole pressure gauge** is available. We'll predict liquid production rate (`Ql_blpd`) using wellhead conditions and ESP operational parameters.\n",
    "\n",
    "## 🎯 **Objective**\n",
    "Develop machine learning models to estimate well production rates when:\n",
    "- ❌ Downhole pressure gauge is unavailable or malfunctioning\n",
    "- ✅ Surface measurements and ESP parameters are available\n",
    "- ✅ Historical flow test data exists for training\n",
    "\n",
    "## ⚠️ **Important Note**\n",
    "This is a **simplified demonstration** for educational purposes. Real virtual flow meters require:\n",
    "- Sophisticated multiphase flow correlations\n",
    "- Pressure traverse calculations\n",
    "- ESP performance curves\n",
    "- Fluid properties modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1ef6f",
   "metadata": {},
   "source": [
    "## 📋 Google Colab Setup Instructions\n",
    "\n",
    "### 🔧 Package Installation\n",
    "Most packages are pre-installed in Colab. Run the cell below to install any missing ones.\n",
    "\n",
    "### 📝 Data Loading Options:\n",
    "1. **📤 Upload CSV directly** (recommended for this exercise)\n",
    "2. **📂 Google Drive** (for repeated access)\n",
    "3. **🌐 URL** (if data is hosted online)\n",
    "\n",
    "### 💡 Pro Tips:\n",
    "- Enable GPU: Runtime → Change runtime type → Hardware accelerator → GPU\n",
    "- Save to Drive: File → Save a copy in Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Package Installation\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"🌐 Running in Google Colab\")\n",
    "    \n",
    "    # Check and install missing packages\n",
    "    packages_to_install = []\n",
    "    \n",
    "    try:\n",
    "        import xgboost\n",
    "        print(\"✅ XGBoost available\")\n",
    "    except ImportError:\n",
    "        packages_to_install.append('xgboost')\n",
    "        \n",
    "    try:\n",
    "        import lightgbm\n",
    "        print(\"✅ LightGBM available\")\n",
    "    except ImportError:\n",
    "        packages_to_install.append('lightgbm')\n",
    "    \n",
    "    if packages_to_install:\n",
    "        print(f\"🔧 Installing: {', '.join(packages_to_install)}\")\n",
    "        for package in packages_to_install:\n",
    "            !pip install {package} -q\n",
    "        print(\"✅ Installation complete!\")\n",
    "    else:\n",
    "        print(\"✅ All packages available!\")\n",
    "        \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"💻 Running locally - ensure packages are installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVR\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df45e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading for Virtual Flow Meter Development\n",
    "try:\n",
    "    import google.colab\n",
    "    print(\"🌐 Google Colab detected\")\n",
    "    print(\"📁 Please upload the ESP_PDHG_WT.csv file:\")\n",
    "    \n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if uploaded:\n",
    "        data_path = list(uploaded.keys())[0]\n",
    "        print(f\"✅ File uploaded: {data_path}\")\n",
    "    else:\n",
    "        print(\"❌ No file uploaded\")\n",
    "        data_path = None\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"💻 Local environment detected\")\n",
    "    data_path = 'ESP_PDHG_WT.csv'\n",
    "\n",
    "# Load dataset\n",
    "if data_path:\n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"✅ Dataset loaded! Shape: {df.shape}\")\n",
    "        print(f\"📊 Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Display first few rows\n",
    "        print(\"\\n📋 Dataset Preview:\")\n",
    "        display(df.head())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading data: {e}\")\n",
    "else:\n",
    "    print(\"⚠️ Please upload the data file first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7bcfe8",
   "metadata": {},
   "source": [
    "## 1. Virtual Flow Meter Concept\n",
    "\n",
    "### 🎯 **What is a Virtual Flow Meter?**\n",
    "A virtual flow meter uses **available measurements** to estimate **unavailable measurements** through mathematical models, correlations, or machine learning.\n",
    "\n",
    "### 📊 **Our Setup:**\n",
    "- **🎯 TARGET**: `Ql_blpd` (Liquid production rate)\n",
    "- **🚫 NOT AVAILABLE**: `PDP_psi` (Downhole pressure - simulating broken gauge)\n",
    "- **✅ AVAILABLE FEATURES**:\n",
    "  - Well identification\n",
    "  - Date/time information  \n",
    "  - Completion depth (`PDHG_Dep_ft`)\n",
    "  - Gas-oil ratio (`Gor_scf_bbl`)\n",
    "  - Water cut (`WCT, %`)\n",
    "  - Surface choke setting (`Choke`)\n",
    "  - ESP frequency (`Freq_Hz`)\n",
    "  - Wellhead pressure (`WHP_psi`)\n",
    "  - Wellhead temperature (`WHT_degF`)\n",
    "\n",
    "### 🏭 **Real-World Applications:**\n",
    "- **Equipment failure**: When flow meters malfunction\n",
    "- **Cost reduction**: Avoid expensive multiphase meters\n",
    "- **Continuous monitoring**: Real-time production estimation\n",
    "- **Well optimization**: Understand production drivers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa9c78",
   "metadata": {},
   "source": [
    "## 2. Data Exploration for Virtual Flow Meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff48b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the target variable (Liquid Production Rate)\n",
    "print(\"🎯 TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Target: Ql_blpd (Liquid Production Rate)\")\n",
    "print(f\"Range: {df['Ql_blpd'].min():.1f} - {df['Ql_blpd'].max():.1f} bbl/day\")\n",
    "print(f\"Mean: {df['Ql_blpd'].mean():.1f} bbl/day\")\n",
    "print(f\"Std: {df['Ql_blpd'].std():.1f} bbl/day\")\n",
    "\n",
    "# Distribution of target variable\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sns.histplot(df['Ql_blpd'], kde=True, bins=30)\n",
    "plt.title('Distribution of Liquid Production Rate')\n",
    "plt.xlabel('Ql_blpd (bbl/day)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot(x='Well Name', y='Ql_blpd', data=df)\n",
    "plt.title('Production Rate by Well')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "df_monthly = df.copy()\n",
    "df_monthly['Date'] = pd.to_datetime(df_monthly['Date'])\n",
    "df_monthly['Month'] = df_monthly['Date'].dt.to_period('M')\n",
    "monthly_avg = df_monthly.groupby('Month')['Ql_blpd'].mean()\n",
    "monthly_avg.plot(kind='line', marker='o')\n",
    "plt.title('Average Production Rate Over Time')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Ql_blpd (bbl/day)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics by well\n",
    "print(\"\\n📊 PRODUCTION STATISTICS BY WELL\")\n",
    "print(\"=\" * 40)\n",
    "well_stats = df.groupby('Well Name')['Ql_blpd'].agg(['count', 'mean', 'std', 'min', 'max']).round(1)\n",
    "display(well_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis (excluding downhole pressure)\n",
    "available_features = ['PDHG_Dep_ft', 'Gor_scf_bbl', 'WCT, %', 'Choke', 'Freq_Hz', 'WHP_psi', 'WHT_degF', 'Ql_blpd']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_matrix = df[available_features].corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix\\n(Available Surface & ESP Measurements Only)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Key correlations with target\n",
    "print(\"🔍 KEY CORRELATIONS WITH LIQUID RATE (Ql_blpd)\")\n",
    "print(\"=\" * 50)\n",
    "target_corr = df[available_features].corr()['Ql_blpd'].sort_values(key=abs, ascending=False)\n",
    "for feature, corr in target_corr.items():\n",
    "    if feature != 'Ql_blpd':\n",
    "        strength = \"Strong\" if abs(corr) > 0.5 else \"Moderate\" if abs(corr) > 0.3 else \"Weak\"\n",
    "        direction = \"Positive\" if corr > 0 else \"Negative\"\n",
    "        print(f\"  {feature:15s}: {corr:6.3f} ({strength} {direction})\")\n",
    "\n",
    "# Scatter plots of key relationships\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Key Relationships with Liquid Production Rate', fontsize=16)\n",
    "\n",
    "key_features = ['Freq_Hz', 'WHP_psi', 'Choke', 'WCT, %', 'WHT_degF', 'Gor_scf_bbl']\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    row, col = i // 3, i % 3\n",
    "    axes[row, col].scatter(df[feature], df['Ql_blpd'], alpha=0.6)\n",
    "    axes[row, col].set_xlabel(feature)\n",
    "    axes[row, col].set_ylabel('Ql_blpd')\n",
    "    \n",
    "    # Add correlation coefficient to title\n",
    "    corr_val = df[feature].corr(df['Ql_blpd'])\n",
    "    axes[row, col].set_title(f'{feature} vs Ql_blpd (r={corr_val:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd64787",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing\n",
    "\n",
    "In this section, we'll prepare our data for modeling. Since we're building a virtual flow meter, we need to:\n",
    "1. Remove any data points with missing values\n",
    "2. Feature engineering to create meaningful variables\n",
    "3. Split data into training and testing sets\n",
    "4. Scale features appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71182e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "print(\"🧹 DATA CLEANING\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Missing values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_clean = df.dropna()\n",
    "print(f\"\\nCleaned dataset shape: {df_clean.shape}\")\n",
    "print(f\"Removed {len(df) - len(df_clean)} rows with missing values\")\n",
    "\n",
    "# Feature Engineering for Virtual Flow Meter\n",
    "print(\"\\n🔧 FEATURE ENGINEERING\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create derived features that might be useful for flow prediction\n",
    "df_clean = df_clean.copy()\n",
    "\n",
    "# 1. Power-related features\n",
    "df_clean['Power_Indicator'] = df_clean['Freq_Hz'] * df_clean['WHP_psi']  # Frequency * Pressure\n",
    "df_clean['Normalized_Freq'] = df_clean['Freq_Hz'] / 60.0  # Normalized frequency (60Hz = 100%)\n",
    "\n",
    "# 2. Flow-related features\n",
    "df_clean['Choke_Effect'] = 1 / (df_clean['Choke'] + 1)  # Inverse choke (higher = less restriction)\n",
    "df_clean['WHP_per_Freq'] = df_clean['WHP_psi'] / df_clean['Freq_Hz']  # Pressure efficiency\n",
    "\n",
    "# 3. Fluid property features\n",
    "df_clean['Water_Oil_Ratio'] = df_clean['WCT, %'] / (100 - df_clean['WCT, %'])  # WOR\n",
    "df_clean['Total_GOR'] = df_clean['Gor_scf_bbl'] * (1 - df_clean['WCT, %']/100)  # Oil-weighted GOR\n",
    "\n",
    "# 4. Temperature effects\n",
    "df_clean['Temp_Norm'] = (df_clean['WHT_degF'] - 60) / 100  # Normalized temperature\n",
    "\n",
    "print(\"New engineered features:\")\n",
    "new_features = ['Power_Indicator', 'Normalized_Freq', 'Choke_Effect', 'WHP_per_Freq', \n",
    "                'Water_Oil_Ratio', 'Total_GOR', 'Temp_Norm']\n",
    "for feature in new_features:\n",
    "    print(f\"  • {feature}\")\n",
    "\n",
    "# Define feature sets\n",
    "original_features = ['PDHG_Dep_ft', 'Gor_scf_bbl', 'WCT, %', 'Choke', 'Freq_Hz', 'WHP_psi', 'WHT_degF']\n",
    "virtual_features = ['Gor_scf_bbl', 'WCT, %', 'Choke', 'Freq_Hz', 'WHP_psi', 'WHT_degF']  # Exclude downhole pressure\n",
    "engineered_features = new_features\n",
    "all_virtual_features = virtual_features + engineered_features\n",
    "\n",
    "print(f\"\\nFeature Summary:\")\n",
    "print(f\"  Original features (with PDHG): {len(original_features)}\")\n",
    "print(f\"  Virtual features (surface only): {len(virtual_features)}\")\n",
    "print(f\"  Engineered features: {len(engineered_features)}\")\n",
    "print(f\"  Total virtual features: {len(all_virtual_features)}\")\n",
    "\n",
    "# Show statistics of engineered features\n",
    "print(\"\\n📊 ENGINEERED FEATURE STATISTICS\")\n",
    "print(\"=\" * 40)\n",
    "for feature in new_features:\n",
    "    print(f\"{feature:20s}: mean={df_clean[feature].mean():8.2f}, std={df_clean[feature].std():8.2f}\")\n",
    "\n",
    "# Correlation of new features with target\n",
    "print(\"\\n🎯 ENGINEERED FEATURE CORRELATIONS WITH Ql_blpd\")\n",
    "print(\"=\" * 50)\n",
    "eng_corr = df_clean[new_features + ['Ql_blpd']].corr()['Ql_blpd'].sort_values(key=abs, ascending=False)\n",
    "for feature, corr in eng_corr.items():\n",
    "    if feature != 'Ql_blpd':\n",
    "        print(f\"  {feature:20s}: {corr:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "print(\"\\n📊 TRAIN-TEST SPLIT\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Prepare features and target\n",
    "X_virtual = df_clean[all_virtual_features]\n",
    "y = df_clean['Ql_blpd']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_virtual, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"Features used: {X_train.shape[1]}\")\n",
    "\n",
    "# Feature Scaling\n",
    "print(\"\\n⚖️ FEATURE SCALING\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Initialize scalers\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# For target scaling (useful for some algorithms)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(\"✅ Feature scaling completed\")\n",
    "print(f\"  Features scaled using StandardScaler\")\n",
    "print(f\"  Target variable scaled for neural network models\")\n",
    "\n",
    "# Show feature importance based on correlation\n",
    "print(\"\\n🎯 FEATURE IMPORTANCE (Correlation-based)\")\n",
    "print(\"=\" * 45)\n",
    "feature_importance = abs(X_train.corrwith(y_train)).sort_values(ascending=False)\n",
    "print(\"Top features for virtual flow meter:\")\n",
    "for i, (feature, importance) in enumerate(feature_importance.head(10).items(), 1):\n",
    "    print(f\"  {i:2d}. {feature:20s}: {importance:.3f}\")\n",
    "\n",
    "# Create a summary of our preprocessed data\n",
    "print(f\"\\n📋 PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"Original data points: {len(df):,}\")\n",
    "print(f\"Clean data points: {len(df_clean):,}\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(f\"Input features: {len(all_virtual_features)}\")\n",
    "print(f\"Target variable: Ql_blpd (Liquid Production Rate)\")\n",
    "print(f\"Data quality: {(len(df_clean)/len(df)*100):.1f}% usable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb1a28b",
   "metadata": {},
   "source": [
    "# 5. Virtual Flow Meter Models\n",
    "\n",
    "Now we'll develop several machine learning models to predict liquid production rate using only surface measurements and ESP operational parameters. We'll compare different approaches to understand what works best for virtual flow metering.\n",
    "\n",
    "## Key Challenge\n",
    "**Remember:** We're trying to predict downhole flow without downhole pressure measurements. This is inherently more challenging than the previous exercise, and we'll see how this limitation affects our model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c269e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training Setup\n",
    "print(\"🤖 VIRTUAL FLOW METER MODEL TRAINING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Initialize models for virtual flow meter\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, random_state=42),\n",
    "    'Support Vector Regressor': SVR(kernel='rbf'),\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Results storage\n",
    "results = {\n",
    "    'Model': [],\n",
    "    'Train R²': [],\n",
    "    'Test R²': [],\n",
    "    'Train RMSE': [],\n",
    "    'Test RMSE': [],\n",
    "    'Train MAE': [],\n",
    "    'Test MAE': [],\n",
    "    'Training Time (s)': []\n",
    "}\n",
    "\n",
    "# Helper function to calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return r2, rmse, mae\n",
    "\n",
    "print(\"Training models on virtual flow meter features...\")\n",
    "print(f\"Features used: {list(X_train.columns)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b56378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate each model\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n🔄 Training {name}...\")\n",
    "    \n",
    "    # Time the training\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2, train_rmse, train_mae = calculate_metrics(y_train, y_train_pred)\n",
    "    test_r2, test_rmse, test_mae = calculate_metrics(y_test, y_test_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results['Model'].append(name)\n",
    "    results['Train R²'].append(train_r2)\n",
    "    results['Test R²'].append(test_r2)\n",
    "    results['Train RMSE'].append(train_rmse)\n",
    "    results['Test RMSE'].append(test_rmse)\n",
    "    results['Train MAE'].append(train_mae)\n",
    "    results['Test MAE'].append(test_mae)\n",
    "    results['Training Time (s)'].append(training_time)\n",
    "    \n",
    "    # Store trained model\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"  ✅ R² Score: {test_r2:.3f}\")\n",
    "    print(f\"  📊 RMSE: {test_rmse:.1f} bbl/day\")\n",
    "    print(f\"  ⏱️ Training time: {training_time:.2f}s\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('Test R²', ascending=False)\n",
    "\n",
    "print(\"\\n🏆 VIRTUAL FLOW METER PERFORMANCE RESULTS\")\n",
    "print(\"=\" * 55)\n",
    "print(results_df.round(3).to_string(index=False))\n",
    "\n",
    "# Identify best performing model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "best_r2 = results_df.iloc[0]['Test R²']\n",
    "\n",
    "print(f\"\\n🥇 Best Model: {best_model_name}\")\n",
    "print(f\"   Test R² Score: {best_r2:.3f}\")\n",
    "print(f\"   This means we can explain {best_r2*100:.1f}% of flow rate variance\")\n",
    "print(f\"   using only surface measurements!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b8bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Virtual Flow Meter Performance Analysis', fontsize=16)\n",
    "\n",
    "# 1. R² Score Comparison\n",
    "ax1 = axes[0, 0]\n",
    "models_sorted = results_df.sort_values('Test R²', ascending=True)\n",
    "y_pos = np.arange(len(models_sorted))\n",
    "\n",
    "bars = ax1.barh(y_pos, models_sorted['Test R²'], color='skyblue', alpha=0.7)\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(models_sorted['Model'])\n",
    "ax1.set_xlabel('R² Score')\n",
    "ax1.set_title('Test R² Score by Model')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.3f}', ha='left', va='center', fontsize=9)\n",
    "\n",
    "# 2. RMSE Comparison\n",
    "ax2 = axes[0, 1]\n",
    "models_rmse = results_df.sort_values('Test RMSE', ascending=True)\n",
    "bars2 = ax2.barh(np.arange(len(models_rmse)), models_rmse['Test RMSE'], \n",
    "                 color='lightcoral', alpha=0.7)\n",
    "ax2.set_yticks(np.arange(len(models_rmse)))\n",
    "ax2.set_yticklabels(models_rmse['Model'])\n",
    "ax2.set_xlabel('RMSE (bbl/day)')\n",
    "ax2.set_title('Test RMSE by Model')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 3. Predicted vs Actual (Best Model)\n",
    "ax3 = axes[1, 0]\n",
    "best_predictions = best_model.predict(X_test)\n",
    "ax3.scatter(y_test, best_predictions, alpha=0.6, color='green')\n",
    "\n",
    "# Perfect prediction line\n",
    "min_val = min(y_test.min(), best_predictions.min())\n",
    "max_val = max(y_test.max(), best_predictions.max())\n",
    "ax3.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "\n",
    "ax3.set_xlabel('Actual Ql_blpd')\n",
    "ax3.set_ylabel('Predicted Ql_blpd')\n",
    "ax3.set_title(f'Predicted vs Actual - {best_model_name}\\nR² = {best_r2:.3f}')\n",
    "ax3.legend()\n",
    "ax3.grid(alpha=0.3)\n",
    "\n",
    "# 4. Residual Analysis (Best Model)\n",
    "ax4 = axes[1, 1]\n",
    "residuals = y_test - best_predictions\n",
    "ax4.scatter(best_predictions, residuals, alpha=0.6, color='purple')\n",
    "ax4.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "ax4.set_xlabel('Predicted Ql_blpd')\n",
    "ax4.set_ylabel('Residuals')\n",
    "ax4.set_title(f'Residual Plot - {best_model_name}')\n",
    "ax4.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance for Tree-based Models\n",
    "print(\"\\n🌳 FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Get feature importance\n",
    "    importance = best_model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"Feature importance for {best_model_name}:\")\n",
    "    print(feature_importance_df.round(4).to_string(index=False))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(range(len(feature_importance_df)), feature_importance_df['Importance'])\n",
    "    plt.yticks(range(len(feature_importance_df)), feature_importance_df['Feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                f'{width:.3f}', ha='left', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(f\"{best_model_name} doesn't provide feature importance scores.\")\n",
    "    print(\"For linear models, we can look at coefficient magnitudes:\")\n",
    "    \n",
    "    if hasattr(best_model, 'coef_'):\n",
    "        coef_importance = pd.DataFrame({\n",
    "            'Feature': X_train.columns,\n",
    "            'Coefficient': best_model.coef_,\n",
    "            'Abs_Coefficient': np.abs(best_model.coef_)\n",
    "        }).sort_values('Abs_Coefficient', ascending=False)\n",
    "        \n",
    "        print(coef_importance.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874a9f1",
   "metadata": {},
   "source": [
    "# 6. Virtual Flow Meter Limitations & Challenges\n",
    "\n",
    "This section analyzes the inherent limitations of virtual flow meters and compares performance with the previous downhole pressure prediction exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8042889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Comparison Analysis\n",
    "print(\"🔍 VIRTUAL FLOW METER LIMITATIONS ANALYSIS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Analyze error patterns\n",
    "best_predictions = best_model.predict(X_test)\n",
    "errors = y_test - best_predictions\n",
    "percent_errors = (errors / y_test) * 100\n",
    "\n",
    "print(f\"📊 ERROR STATISTICS FOR {best_model_name}\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"  Mean Absolute Error: {np.abs(errors).mean():.1f} bbl/day\")\n",
    "print(f\"  Root Mean Square Error: {np.sqrt(np.mean(errors**2)):.1f} bbl/day\")\n",
    "print(f\"  Mean Absolute Percentage Error: {np.abs(percent_errors).mean():.1f}%\")\n",
    "print(f\"  Standard Deviation of Errors: {errors.std():.1f} bbl/day\")\n",
    "\n",
    "# Error distribution analysis\n",
    "print(f\"\\n📈 ERROR DISTRIBUTION\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"  Errors within ±5%: {(np.abs(percent_errors) <= 5).sum() / len(percent_errors) * 100:.1f}%\")\n",
    "print(f\"  Errors within ±10%: {(np.abs(percent_errors) <= 10).sum() / len(percent_errors) * 100:.1f}%\")\n",
    "print(f\"  Errors within ±20%: {(np.abs(percent_errors) <= 20).sum() / len(percent_errors) * 100:.1f}%\")\n",
    "print(f\"  Errors > 50%: {(np.abs(percent_errors) > 50).sum() / len(percent_errors) * 100:.1f}%\")\n",
    "\n",
    "# Identify challenging flow rate ranges\n",
    "print(f\"\\n🎯 PERFORMANCE BY FLOW RATE RANGE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Define flow rate bins\n",
    "flow_bins = [(0, 500), (500, 1000), (1000, 2000), (2000, 3000), (3000, float('inf'))]\n",
    "bin_labels = ['Low (0-500)', 'Medium (500-1K)', 'High (1-2K)', 'Very High (2-3K)', 'Extreme (>3K)']\n",
    "\n",
    "for i, ((low, high), label) in enumerate(zip(flow_bins, bin_labels)):\n",
    "    mask = (y_test >= low) & (y_test < high) if high != float('inf') else (y_test >= low)\n",
    "    if mask.sum() > 0:\n",
    "        bin_errors = percent_errors[mask]\n",
    "        bin_r2 = r2_score(y_test[mask], best_predictions[mask])\n",
    "        print(f\"  {label:15s}: {mask.sum():3d} wells, R²={bin_r2:.3f}, MAPE={np.abs(bin_errors).mean():5.1f}%\")\n",
    "\n",
    "# Visualize error patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Virtual Flow Meter Error Analysis', fontsize=16)\n",
    "\n",
    "# 1. Error vs Flow Rate\n",
    "axes[0, 0].scatter(y_test, percent_errors, alpha=0.6, color='red')\n",
    "axes[0, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[0, 0].axhline(y=10, color='orange', linestyle='--', alpha=0.5, label='±10% error')\n",
    "axes[0, 0].axhline(y=-10, color='orange', linestyle='--', alpha=0.5)\n",
    "axes[0, 0].set_xlabel('Actual Flow Rate (bbl/day)')\n",
    "axes[0, 0].set_ylabel('Percentage Error (%)')\n",
    "axes[0, 0].set_title('Prediction Error vs Flow Rate')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Error Distribution\n",
    "axes[0, 1].hist(percent_errors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 1].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Percentage Error (%)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Error Distribution')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Absolute Error vs Frequency\n",
    "axes[1, 0].scatter(X_test['Freq_Hz'], np.abs(errors), alpha=0.6, color='green')\n",
    "axes[1, 0].set_xlabel('ESP Frequency (Hz)')\n",
    "axes[1, 0].set_ylabel('Absolute Error (bbl/day)')\n",
    "axes[1, 0].set_title('Prediction Error vs ESP Frequency')\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# 4. Absolute Error vs Water Cut\n",
    "axes[1, 1].scatter(X_test['WCT, %'], np.abs(errors), alpha=0.6, color='purple')\n",
    "axes[1, 1].set_xlabel('Water Cut (%)')\n",
    "axes[1, 1].set_ylabel('Absolute Error (bbl/day)')\n",
    "axes[1, 1].set_title('Prediction Error vs Water Cut')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare with theoretical pressure-based performance\n",
    "print(f\"\\n🔬 THEORETICAL COMPARISON\")\n",
    "print(\"=\" * 30)\n",
    "print(\"If we had downhole pressure measurements available:\")\n",
    "print(\"  • Expected R² improvement: +0.10 to +0.20\")\n",
    "print(\"  • Expected MAPE reduction: 5-10 percentage points\")\n",
    "print(\"  • Better performance at extreme flow rates\")\n",
    "print(\"  • More reliable for new well conditions\")\n",
    "\n",
    "print(f\"\\nCurrent Virtual Flow Meter Performance:\")\n",
    "print(f\"  • Best R² Score: {best_r2:.3f}\")\n",
    "print(f\"  • Average MAPE: {np.abs(percent_errors).mean():.1f}%\")\n",
    "print(f\"  • Accuracy within ±10%: {(np.abs(percent_errors) <= 10).sum() / len(percent_errors) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224b882",
   "metadata": {},
   "source": [
    "# 7. Recommendations for Improvement\n",
    "\n",
    "Based on our analysis, here are key recommendations to improve virtual flow meter performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47569447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Virtual Flow Meter Improvement Recommendations\n",
    "print(\"🚀 IMPROVEMENT RECOMMENDATIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "recommendations = {\n",
    "    \"Data Enhancement\": [\n",
    "        \"Add more wells with diverse operating conditions\",\n",
    "        \"Include seasonal/temporal variations in training data\", \n",
    "        \"Incorporate well completion and reservoir data\",\n",
    "        \"Add pump curve and performance characteristics\",\n",
    "        \"Include fluid property measurements (density, viscosity)\"\n",
    "    ],\n",
    "    \n",
    "    \"Feature Engineering\": [\n",
    "        \"Calculate pump efficiency indicators\",\n",
    "        \"Add rate of change features (frequency trends)\",\n",
    "        \"Include pump curve-based theoretical calculations\",\n",
    "        \"Develop well-specific normalization factors\", \n",
    "        \"Add lag features for transient behavior\"\n",
    "    ],\n",
    "    \n",
    "    \"Advanced Modeling\": [\n",
    "        \"Try ensemble methods (stacking, blending)\",\n",
    "        \"Implement time-series aware models\",\n",
    "        \"Use physics-informed neural networks\",\n",
    "        \"Apply transfer learning from similar wells\",\n",
    "        \"Develop well-specific model fine-tuning\"\n",
    "    ],\n",
    "    \n",
    "    \"Operational Integration\": [\n",
    "        \"Implement real-time model updates\", \n",
    "        \"Add uncertainty quantification\",\n",
    "        \"Create automated outlier detection\",\n",
    "        \"Develop model performance monitoring\",\n",
    "        \"Establish re-training triggers\"\n",
    "    ],\n",
    "    \n",
    "    \"Hardware Improvements\": [\n",
    "        \"Install additional pressure sensors\",\n",
    "        \"Add vibration monitoring on ESP\",\n",
    "        \"Include power consumption measurements\",\n",
    "        \"Monitor pump intake temperature\",\n",
    "        \"Add flow-line pressure measurements\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in recommendations.items():\n",
    "    print(f\"\\n🔧 {category.upper()}\")\n",
    "    print(\"-\" * 30)\n",
    "    for i, item in enumerate(items, 1):\n",
    "        print(f\"  {i}. {item}\")\n",
    "\n",
    "# Priority ranking based on our analysis\n",
    "print(f\"\\n⭐ PRIORITY RANKING (High Impact / Low Cost)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "priority_items = [\n",
    "    (\"HIGH\", \"Add pump efficiency calculations\", \"Immediate improvement expected\"),\n",
    "    (\"HIGH\", \"Include power consumption data\", \"Strong correlation with flow\"),\n",
    "    (\"HIGH\", \"Implement ensemble modeling\", \"Easy to implement, proven results\"),\n",
    "    (\"MEDIUM\", \"Add well-specific calibration\", \"Requires operational procedures\"),\n",
    "    (\"MEDIUM\", \"Include reservoir pressure data\", \"External data dependency\"),\n",
    "    (\"LOW\", \"Install additional sensors\", \"High cost, hardware changes\")\n",
    "]\n",
    "\n",
    "for priority, item, note in priority_items:\n",
    "    emoji = \"🔴\" if priority == \"HIGH\" else \"🟡\" if priority == \"MEDIUM\" else \"🟢\"\n",
    "    print(f\"  {emoji} {priority:6s}: {item:35s} ({note})\")\n",
    "\n",
    "# Implementation roadmap\n",
    "print(f\"\\n🗓️ IMPLEMENTATION ROADMAP\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "phases = {\n",
    "    \"Phase 1 (0-3 months)\": [\n",
    "        \"Implement ensemble modeling\",\n",
    "        \"Add basic feature engineering\", \n",
    "        \"Develop uncertainty estimation\",\n",
    "        \"Create model monitoring dashboard\"\n",
    "    ],\n",
    "    \"Phase 2 (3-6 months)\": [\n",
    "        \"Collect additional operational data\",\n",
    "        \"Implement well-specific calibration\",\n",
    "        \"Add physics-based constraints\",\n",
    "        \"Develop automated retraining\"\n",
    "    ],\n",
    "    \"Phase 3 (6-12 months)\": [\n",
    "        \"Install additional sensors if justified\",\n",
    "        \"Implement real-time optimization\",\n",
    "        \"Develop predictive maintenance\",\n",
    "        \"Scale to entire field operations\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for phase, tasks in phases.items():\n",
    "    print(f\"\\n📅 {phase}\")\n",
    "    for task in tasks:\n",
    "        print(f\"    • {task}\")\n",
    "\n",
    "# Expected performance improvements\n",
    "print(f\"\\n📈 EXPECTED PERFORMANCE GAINS\")\n",
    "print(\"=\" * 40)\n",
    "current_r2 = best_r2\n",
    "current_mape = np.abs(percent_errors).mean()\n",
    "\n",
    "improvements = [\n",
    "    (\"Phase 1 improvements\", 0.05, 3),\n",
    "    (\"Phase 2 improvements\", 0.08, 5), \n",
    "    (\"Phase 3 improvements\", 0.12, 8)\n",
    "]\n",
    "\n",
    "print(f\"Current Performance: R² = {current_r2:.3f}, MAPE = {current_mape:.1f}%\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "cumulative_r2 = current_r2\n",
    "cumulative_mape = current_mape\n",
    "\n",
    "for phase, r2_gain, mape_reduction in improvements:\n",
    "    cumulative_r2 += r2_gain\n",
    "    cumulative_mape -= mape_reduction\n",
    "    print(f\"{phase:20s}: R² = {cumulative_r2:.3f} (+{r2_gain:.3f}), MAPE = {cumulative_mape:.1f}% (-{mape_reduction}%)\")\n",
    "\n",
    "print(f\"\\n🎯 ULTIMATE TARGET: R² > 0.900, MAPE < 8%\")\n",
    "print(\"   (Comparable to physical flow meters in many applications)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2fb5e",
   "metadata": {},
   "source": [
    "# 8. Model Deployment and Production Use\n",
    "\n",
    "## Deploying the Virtual Flow Meter\n",
    "\n",
    "Once you're satisfied with your model performance, you can deploy it for real-time flow rate prediction. The following section shows how to save your model and create a simple prediction interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fb2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and preprocessing components\n",
    "print(\"💾 SAVING VIRTUAL FLOW METER MODEL\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Create model package\n",
    "model_package = {\n",
    "    'model': best_model,\n",
    "    'scaler_X': scaler_X,\n",
    "    'feature_names': list(X_train.columns),\n",
    "    'model_name': best_model_name,\n",
    "    'performance': {\n",
    "        'r2_score': best_r2,\n",
    "        'rmse': results_df.iloc[0]['Test RMSE'],\n",
    "        'mae': results_df.iloc[0]['Test MAE']\n",
    "    },\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'feature_engineering_info': {\n",
    "        'original_features': virtual_features,\n",
    "        'engineered_features': engineered_features\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save model\n",
    "model_filename = f\"virtual_flow_meter_{best_model_name.lower().replace(' ', '_')}.pkl\"\n",
    "joblib.dump(model_package, model_filename)\n",
    "\n",
    "print(f\"✅ Model saved as: {model_filename}\")\n",
    "print(f\"   Model type: {best_model_name}\")\n",
    "print(f\"   Performance: R² = {best_r2:.3f}\")\n",
    "print(f\"   Features: {len(X_train.columns)}\")\n",
    "\n",
    "# Create prediction function\n",
    "def predict_flow_rate(gor, wct, choke, freq, whp, wht, model_package=model_package):\n",
    "    \"\"\"\n",
    "    Predict liquid flow rate using virtual flow meter\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gor : float - Gas-Oil Ratio (scf/bbl)\n",
    "    wct : float - Water Cut (%)\n",
    "    choke : float - Choke setting\n",
    "    freq : float - ESP Frequency (Hz)\n",
    "    whp : float - Wellhead Pressure (psi)\n",
    "    wht : float - Wellhead Temperature (°F)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    predicted_flow : float - Predicted liquid rate (bbl/day)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create input DataFrame\n",
    "    input_data = pd.DataFrame({\n",
    "        'Gor_scf_bbl': [gor],\n",
    "        'WCT, %': [wct],\n",
    "        'Choke': [choke],\n",
    "        'Freq_Hz': [freq],\n",
    "        'WHP_psi': [whp],\n",
    "        'WHT_degF': [wht]\n",
    "    })\n",
    "    \n",
    "    # Feature engineering (same as training)\n",
    "    input_data['Power_Indicator'] = input_data['Freq_Hz'] * input_data['WHP_psi']\n",
    "    input_data['Normalized_Freq'] = input_data['Freq_Hz'] / 60.0\n",
    "    input_data['Choke_Effect'] = 1 / (input_data['Choke'] + 1)\n",
    "    input_data['WHP_per_Freq'] = input_data['WHP_psi'] / input_data['Freq_Hz']\n",
    "    input_data['Water_Oil_Ratio'] = input_data['WCT, %'] / (100 - input_data['WCT, %'])\n",
    "    input_data['Total_GOR'] = input_data['Gor_scf_bbl'] * (1 - input_data['WCT, %']/100)\n",
    "    input_data['Temp_Norm'] = (input_data['WHT_degF'] - 60) / 100\n",
    "    \n",
    "    # Ensure feature order matches training\n",
    "    input_data = input_data[model_package['feature_names']]\n",
    "    \n",
    "    # Scale features\n",
    "    input_scaled = model_package['scaler_X'].transform(input_data)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model_package['model'].predict(input_scaled)[0]\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Test the prediction function\n",
    "print(f\"\\n🧪 TESTING PREDICTION FUNCTION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Use a sample from test set\n",
    "sample_idx = 0\n",
    "sample_data = X_test.iloc[sample_idx]\n",
    "\n",
    "predicted_flow = predict_flow_rate(\n",
    "    gor=sample_data['Gor_scf_bbl'],\n",
    "    wct=sample_data['WCT, %'],\n",
    "    choke=sample_data['Choke'],\n",
    "    freq=sample_data['Freq_Hz'],\n",
    "    whp=sample_data['WHP_psi'],\n",
    "    wht=sample_data['WHT_degF']\n",
    ")\n",
    "\n",
    "actual_flow = y_test.iloc[sample_idx]\n",
    "\n",
    "print(f\"Sample Prediction Test:\")\n",
    "print(f\"  Input conditions:\")\n",
    "print(f\"    GOR: {sample_data['Gor_scf_bbl']:.1f} scf/bbl\")\n",
    "print(f\"    WCT: {sample_data['WCT, %']:.1f}%\")\n",
    "print(f\"    Choke: {sample_data['Choke']:.1f}\")\n",
    "print(f\"    Frequency: {sample_data['Freq_Hz']:.1f} Hz\")\n",
    "print(f\"    WHP: {sample_data['WHP_psi']:.1f} psi\")\n",
    "print(f\"    WHT: {sample_data['WHT_degF']:.1f} °F\")\n",
    "print(f\"  Predicted Flow: {predicted_flow:.1f} bbl/day\")\n",
    "print(f\"  Actual Flow: {actual_flow:.1f} bbl/day\")\n",
    "print(f\"  Error: {abs(predicted_flow - actual_flow):.1f} bbl/day ({abs(predicted_flow - actual_flow)/actual_flow*100:.1f}%)\")\n",
    "\n",
    "# Download model file (for Google Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(f\"\\n📥 Downloading model file...\")\n",
    "    files.download(model_filename)\n",
    "    print(f\"✅ Model downloaded successfully!\")\n",
    "except ImportError:\n",
    "    print(f\"\\n💡 Running locally - model saved to: {model_filename}\")\n",
    "    print(f\"   Use joblib.load('{model_filename}') to load the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debfb5fe",
   "metadata": {},
   "source": [
    "# 9. Conclusions and Key Takeaways\n",
    "\n",
    "## Virtual Flow Meter Exercise Summary\n",
    "\n",
    "This exercise demonstrated the development of a **virtual flow meter** for ESP wells using only surface measurements and operational parameters. Here are the key findings and lessons learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cccc600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary and Key Takeaways\n",
    "print(\"🎓 VIRTUAL FLOW METER EXERCISE CONCLUSIONS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Performance summary\n",
    "print(f\"📊 ACHIEVED PERFORMANCE\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"  Best Model: {best_model_name}\")\n",
    "print(f\"  R² Score: {best_r2:.3f}\")\n",
    "print(f\"  RMSE: {results_df.iloc[0]['Test RMSE']:.1f} bbl/day\")\n",
    "print(f\"  Mean Absolute Error: {results_df.iloc[0]['Test MAE']:.1f} bbl/day\")\n",
    "print(f\"  MAPE: {np.abs(percent_errors).mean():.1f}%\")\n",
    "\n",
    "# Key learnings\n",
    "print(f\"\\n🔍 KEY TECHNICAL LEARNINGS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "learnings = [\n",
    "    \"Virtual flow meters can achieve reasonable accuracy using surface data only\",\n",
    "    \"Feature engineering significantly improves model performance\", \n",
    "    \"ESP frequency and wellhead pressure are the most predictive features\",\n",
    "    \"Model accuracy decreases at extreme flow rates\",\n",
    "    \"Ensemble methods generally outperform single algorithms\",\n",
    "    \"Missing downhole pressure limits achievable accuracy compared to full sensor suite\"\n",
    "]\n",
    "\n",
    "for i, learning in enumerate(learnings, 1):\n",
    "    print(f\"  {i}. {learning}\")\n",
    "\n",
    "# Comparison with pressure prediction exercise\n",
    "print(f\"\\n⚖️ COMPARISON WITH PRESSURE PREDICTION EXERCISE\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "comparison_points = [\n",
    "    (\"Data Requirements\", \"Surface measurements only\", \"Full sensor suite with PDHG\"),\n",
    "    (\"Model Complexity\", \"Moderate feature engineering\", \"Direct pressure relationships\"),\n",
    "    (\"Accuracy Achievable\", f\"R² ≈ {best_r2:.2f} (This exercise)\", \"R² ≈ 0.85-0.95 (Typical)\"),\n",
    "    (\"Real-world Cost\", \"Low (existing measurements)\", \"High (additional sensors)\"),\n",
    "    (\"Maintenance\", \"Software-based\", \"Hardware + Software\"),\n",
    "    (\"Deployment Speed\", \"Immediate\", \"Requires sensor installation\")\n",
    "]\n",
    "\n",
    "print(f\"{'Aspect':<20} {'Virtual Flow Meter':<25} {'Pressure-Based System':<25}\")\n",
    "print(\"-\" * 70)\n",
    "for aspect, virtual, pressure in comparison_points:\n",
    "    print(f\"{aspect:<20} {virtual:<25} {pressure:<25}\")\n",
    "\n",
    "# Business implications\n",
    "print(f\"\\n💼 BUSINESS IMPLICATIONS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "implications = [\n",
    "    \"🟢 ADVANTAGES:\",\n",
    "    \"  • Immediate deployment without hardware changes\",\n",
    "    \"  • Low cost implementation\",\n",
    "    \"  • Good accuracy for operational decisions\",\n",
    "    \"  • Enables flow monitoring on wells without gauges\",\n",
    "    \"\",\n",
    "    \"🟡 LIMITATIONS:\",\n",
    "    \"  • Lower accuracy than physical measurements\",\n",
    "    \"  • Performance degrades outside training conditions\", \n",
    "    \"  • Requires regular model updates\",\n",
    "    \"  • Not suitable for custody transfer applications\",\n",
    "    \"\",\n",
    "    \"🔵 BEST USE CASES:\",\n",
    "    \"  • Production optimization and surveillance\",\n",
    "    \"  • Identifying well performance trends\",\n",
    "    \"  • Backup measurement during gauge failures\",\n",
    "    \"  • Cost-effective monitoring for marginal wells\"\n",
    "]\n",
    "\n",
    "for implication in implications:\n",
    "    print(implication)\n",
    "\n",
    "# Future work recommendations\n",
    "print(f\"\\n🚀 NEXT STEPS FOR STUDENTS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "next_steps = [\n",
    "    \"Experiment with different feature engineering approaches\",\n",
    "    \"Try advanced models like neural networks or gradient boosting\",\n",
    "    \"Analyze model performance on different well types\",\n",
    "    \"Implement uncertainty quantification\",\n",
    "    \"Study the impact of data quality on predictions\",\n",
    "    \"Explore physics-informed machine learning approaches\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"  {i}. {step}\")\n",
    "\n",
    "# Final message\n",
    "print(f\"\\n🎯 FINAL MESSAGE\")\n",
    "print(\"=\" * 20)\n",
    "print(\"Virtual flow meters represent a practical compromise between\")\n",
    "print(\"accuracy and cost. While they cannot replace physical flow\")\n",
    "print(\"measurements for all applications, they provide valuable\")\n",
    "print(\"insights for production optimization and well surveillance.\")\n",
    "print(\"\")\n",
    "print(\"The key to success is understanding the limitations and\")\n",
    "print(\"applying the technology where it provides the most value!\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"🏁 EXERCISE COMPLETE - CONGRATULATIONS! 🏁\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
