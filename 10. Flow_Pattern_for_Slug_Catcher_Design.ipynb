{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v7lPoOZVpBr",
        "outputId": "142dcada-5225-4d11-fb96-5a63429e740d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfOT05WaVebs"
      },
      "source": [
        "# Exploring the database\n",
        "\n",
        "Import relevant packages and increase the display length when printing using `pandas` to `160`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcrq7y1fVebu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.options.display.width = 160"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VuWb5srVebv"
      },
      "source": [
        "## Reading the data\n",
        "\n",
        "Read the `pickle` file which contains the database and show the first elements of the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "QkJOcG6SVebw",
        "outputId": "f8fd8e52-793f-47a6-ef0e-dde11fdb059b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df = pd.read_pickle('/content/drive/MyDrive/ALCE/SPE_Database.pkl')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4CYkm3pVebx"
      },
      "source": [
        "If your data is in another format, you can use other reading methods from the `pandas` library. For instance, if your data was in an Excel spreadsheet, you can use the `pd.read_excel` method to read your data and use it as an input to the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmyf6GyLVebx"
      },
      "source": [
        "## Viewing the data\n",
        "\n",
        "The `df.describe()` method gives some insight in the range of values and the distribution of numerical data. You can also explore the dataset by printing `df.columns`, which will show the name of all of the dataframe's columns and exploring the values of each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "g9QPvxwrVebx",
        "outputId": "19dbd47c-b854-4a78-a8de-e818cd458aa2",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYMJN7XSVeby"
      },
      "source": [
        "The `Series.value_counts()` method counts the number of times a given value is present in a dataframe. For our database, the following cell counts the number of examples of a given study. It is seen that the data from Shoham is very significant in this database, with more than 5000 examples, which represents roughly $47\\%$ of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmwwzYgHVeby",
        "outputId": "411e25bf-32c2-4fd3-a703-eadd1d44aa17",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "df['author'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cAzGGisVeby",
        "outputId": "3375d1c4-8878-47e8-d8ad-ad6cd0ec73ed",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "df['author'].value_counts()/len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzXWfTx3Vebz"
      },
      "source": [
        "Similarly, the number of points of each flow pattern can be computed usind the `value_counts` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgnG5Q7dVebz",
        "outputId": "0279c763-915c-42a8-840d-489a8a0d1745"
      },
      "outputs": [],
      "source": [
        "df['flow_pattern_code'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m56ajzwvVebz",
        "outputId": "101a0bd7-9caa-4f6b-b0f7-f367a635b74c"
      },
      "outputs": [],
      "source": [
        "df['flow_pattern_name'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmVgZASNVebz"
      },
      "source": [
        "The cell below plots values of the variables selected in `columns` groupped by the `author` field. It is possible to check which variables were studied for each author. This is done by using a `boxplot`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tAMolR8CVeb0",
        "outputId": "9473a09d-9c5e-483c-91ba-0501fe17b7ba",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "columns = ['v_sl', 'v_sg', 'rho_l', 'rho_g', 'mu_l', 'mu_g', 'D']\n",
        "figsize = (8, 8)\n",
        "\n",
        "for c in columns:\n",
        "    print('-'*120 + f'\\n{c}')\n",
        "    print(df.groupby('author')[c].describe(percentiles=[]))\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    df.boxplot(by='author', column=c, rot=90, ax=ax)\n",
        "    ax.set_yscale('log')\n",
        "    ax.set_ylabel(c)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE3gGJ9oVeb0"
      },
      "source": [
        "Histograms are easy to plot as well using the `pandas` library. The cells below show the liquids and gases present in the database and their frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "C9P5gBwdVeb0",
        "outputId": "d3caba16-2ba9-4dca-a505-57a5825d792d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df['liquid'].hist(xrot=90, bins=len(df.liquid.unique()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "XOfMBhmuVeb0",
        "outputId": "e4df8b69-1029-49c3-c8cf-ca2171dcf1f3",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "df['gas'].hist(xrot=90, bins=len(df.gas.unique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDrweirYVeb1"
      },
      "source": [
        "As a last example of exploring the dataset, the points are grouped by the `author` and `flow_pattern_name` fields and the number of points in this subgroup of the dataframe is printed. For instance, it can be seen that `Abdul-Majeed (1995)` has 22 points with an `Annular` flow pattern. Also, it only has 4 different flow patterns as it does not have any points with `Dispersed Bubble` or `Bubble Flow`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rwnqUoVVeb1",
        "outputId": "b7793a6a-bcff-4aca-e126-ed95088a2bbe",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "for i, df_sub in df.groupby(['author', 'flow_pattern_name']):\n",
        "        print(f'{i[0][0:20]:20} - {i[1][0:20]:20}: {len(df_sub)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjfF-IerVeb1"
      },
      "source": [
        "It can be seen that the dataset covers a wide range of values for some variables (e.g. `v_sl`, `v_sg`), while other variables are underrepresented. This is not a problem if the `model` is being used to `predict` values in a range similar to what is present in the database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAQmRQYqVeb1"
      },
      "source": [
        "# Data Splitting\n",
        "\n",
        "There are many strategies in splitting the database into training, validation and testing datasets. In here, we will present the most simple one of randomly sampling points from the original dataset. A seed is given to ensure repeatibility between different runs of the split and make sure that the points in the test dataset are not involved in the improovement of the `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYNsnIlmVeb2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHzze8jsVeb2"
      },
      "source": [
        "It is easy to sample a given frequency using the `.sample` method. Here, `df` is sampled and $60\\%$ of the points fo to `df_train`. Then, a dataframe with the remaining points is obtained by dropping the points in `df_train` from `df` using the command `df.drop(df_train.index)`. $50\\%$ of these points are randomly sampled and go to `df_val`, while the remaining points which are not in `df_train` or `df_val` go to `df_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-m467dcVeb2"
      },
      "outputs": [],
      "source": [
        "df_train = df.sample(frac=0.6, random_state=42)\n",
        "df_val = df.drop(df_train.index).sample(frac=0.5, random_state=42)\n",
        "df_test = df.drop(df_train.index).drop(df_val.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSVbRcQUVeb2"
      },
      "source": [
        "As a check of the code above, we can check the lengths of each dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1runSBkVeb3",
        "outputId": "5ab452df-b15f-4140-fb5c-17ef31ac695a"
      },
      "outputs": [],
      "source": [
        "N_points = [len(df_train), len(df_val), len(df_test)]\n",
        "print(N_points)\n",
        "print(sum(N_points), len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDI9cepmVeb3"
      },
      "source": [
        "To check if there is an intersection of the data in `df_train`, `df_val` and `df_test`, the `merge` method can be used. The cell below shows that there is no intersection between `df_train` and `df_val`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "8KyUO5_VVeb3",
        "outputId": "f1c41147-9f73-4b2d-8fb3-6978cbc00c3d"
      },
      "outputs": [],
      "source": [
        "pd.merge(df_train, df_val, how='inner', on=df.columns.to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p8yvByVVeb4"
      },
      "source": [
        "# Neural Networks\n",
        "\n",
        "The neural networks can be implemented very easily by using the `keras` API for tensorflow. First, let's import all relevant objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVhGIAHBVeb4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import optimizers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import Callback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLc4HLDpVeb4"
      },
      "source": [
        "It is good to have some kind of feedback during training to make sure the code is running (at least in the first few iterations). Since our dataset is very small, printing at every `epoch` will be too frequent, so the following `class` is created and will be used as a `Callback` in the `model.fit` method. At the end of each epoch (hence the use of `on_epoch_end`), by using this callback, the code will print the desired information only if `epoch` is divisible by `N`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a928YWm7Veb4"
      },
      "outputs": [],
      "source": [
        "class PrintEveryNEpochs(Callback):\n",
        "    def __init__(self, N):\n",
        "        super(PrintEveryNEpochs, self).__init__()\n",
        "        self.N = N\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch % self.N) == 0:\n",
        "            print(\n",
        "            \"Epoch {:6}: Train loss: {:12.8f} Validation loss: {:12.8f} Accuracy: {:6.3f} Validation Accuracy: {:6.3f}\"\n",
        "            .format(epoch, logs[\"loss\"], logs[\"val_loss\"], logs['accuracy'], logs['val_accuracy'])\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up6UwByfVeb5"
      },
      "source": [
        "## Selection of inputs and outputs and preprocessing\n",
        "\n",
        "The `inputs` and `output` variables are lists of the desired inputs and outputs for the neural network. Specifically, these lists contain the name of the columns of `df` which should be used to get input and output values, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2SJLzOgVeb5",
        "outputId": "99cbb6b8-ef85-49c2-f084-df9754363a6a"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDizhMA2Veb5"
      },
      "outputs": [],
      "source": [
        "inputs = ['rho_l', 'rho_g', 'mu_l', 'mu_g', 'sigma', 'D', 'sin_angle', 'cos_angle', 'v_sl', 'v_sg']\n",
        "output = ['flow_pattern_code']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK09G9ygVeb5"
      },
      "source": [
        "It is a good practice to normalize the data so that it is in a nice numerical range and the values are more significant at first sight. This also gives some intuition for the neural network, as a negative value represents a low value in comparison with the dataset, while a given value (e.g. 800 kg/m$^3$) has no immediately obvius significance for the neural network. This is done by using the `scaler` class from `sklearn`. Notice that the normalization is fitted using `df_train` and the transformation is performed on `df_val` and `df_test` using the values obtained from `df_train`. This is important, as it prevent that data coming from `df_test` and `df_val` to be implictly transfered to `df_train`, contaminating the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOz5ROdlVeb5"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(df_train[inputs].values)\n",
        "X_val, X_test = scaler.transform(df_val[inputs].values), scaler.transform(df_test[inputs].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nnHfeikVeb6"
      },
      "source": [
        "We can check that scaling was successful by evaluating the mean and standard deviation of the resulting `X`s. Notice that the `mean` values for the training dataset are on the order of the machine precision, while the values for validation and training are not exactly zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTEPcrSJVeb6",
        "outputId": "d8efba5e-1ff7-4bda-883e-324e71afea50"
      },
      "outputs": [],
      "source": [
        "X_train.mean(axis=0), X_val.mean(axis=0), X_test.mean(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "830RorwgVeb6"
      },
      "source": [
        "Similarly, the standard deviation for each feature is `1` for the training values, while it is only close to 1 for validaiton and test datasets. This is expected, as the values are slightly different in each dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi7o6ng8Veb6",
        "outputId": "bee3e2f8-355a-474b-a53d-567f62f8f50e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "X_train.std(axis=0), X_val.std(axis=0), X_test.std(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjaRF9WldxvX"
      },
      "source": [
        "We can save the scaler for later use by using the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qqr1hTatdhlq"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "if not os.path.exists('Models'):\n",
        "    os.makedirs('Models')\n",
        "with open('Models/scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWdw8l8kVeb6"
      },
      "source": [
        "`df[output].values` are numerical data with each value corresponding to a different flow pattern. In the neural network model, the `Y` value corresponds to the probability of that point being a given flow pattern. Since each point only has one flow pattern, the probability is `1` for that given flow pattern. Hence, we want to transform each element of the `df[output]` column from a numerical value to an array with a value of `1` for the corresponding flow pattern and `0` for the other ones. This is also known as one-hot encoding. This is done by using the 'to_categorical' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7mn5QUFVeb7"
      },
      "outputs": [],
      "source": [
        "Y_train = to_categorical(df_train[output].values)\n",
        "Y_val = to_categorical(df_val[output].values)\n",
        "Y_test = to_categorical(df_test[output].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f8CzSzgVeb7"
      },
      "source": [
        "Notice that the `Y` values now have 6 columns, corresponding to the six different flow patterns of this problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icPb0N2PVeb7",
        "outputId": "029fc350-4d11-497e-e464-d9f831388ae6"
      },
      "outputs": [],
      "source": [
        "Y_train.shape, Y_val.shape, Y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93Qnn5iUVeb7",
        "outputId": "6f755f9e-ff92-4488-cd88-58656f376310"
      },
      "outputs": [],
      "source": [
        "Y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqfJtq76Veb7"
      },
      "source": [
        "## Building the neural networks\n",
        "\n",
        "With the data already split into training, validation and test and the normalizing and one-hot encoding steps completed, the model can be built. In this example, we will use batches of `64` examples, two hidden layers with `16` neurons each and the `tanh` activation function. `fname` is the name of the file where the model will be saved after training. This is useful because there is no need to train the model every time and it can be readily loaded from the file. Dropout rates of $20\\%$ are used in the intermediate layers, while the last layer has a dropout rate of $50\\%$. The number of epochs was selected to be high enough to obtain the learning curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIfuLDAiVeb8"
      },
      "outputs": [],
      "source": [
        "n_inputs = len(inputs)\n",
        "n_flow_patterns = len(df_train.flow_pattern_code.unique())\n",
        "n_batch= 64\n",
        "n_hidden_layers = [16, 16]\n",
        "fname = \"Models/trained_model_16_16.hdf5\"\n",
        "fname = \"Models/trained_model_16_16_dropout.hdf5\"\n",
        "activation_layer = 'tanh'\n",
        "dropout_rate_mid = 0.2\n",
        "dropout_rate_final = 0.5\n",
        "n_epochs = int(50e3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS17rl8fVeb8"
      },
      "source": [
        "The last activation function is `'softmax'`. This is because we want to interpret the values given by the `model` as a probability for each flow pattern and this activation funtion already normalizes the outputs so they can be interpreted that way. The model is plotted to present the dataflow. A summary of the parameters and shapes is also given."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OYfFLEjgVeb8",
        "outputId": "755e6f23-bbc9-4208-cb28-d1d3ed29ce50"
      },
      "outputs": [],
      "source": [
        "main_input = Input(shape=(n_inputs,), name='main_input')\n",
        "x = main_input\n",
        "while len(n_hidden_layers) > 1:\n",
        "    x = Dense(units=n_hidden_layers.pop(0), activation=activation_layer)(x)\n",
        "    x = Dropout(dropout_rate_mid)(x)\n",
        "x = Dense(units=n_hidden_layers.pop(0), activation='relu')(x)\n",
        "x = Dropout(dropout_rate_final)(x)\n",
        "flow_pattern = Dense(units=n_flow_patterns, activation='softmax', name='flow_pattern')(x)\n",
        "\n",
        "#%% Model build\n",
        "model = Model(inputs=[main_input], outputs=[flow_pattern])\n",
        "\n",
        "plot_model(model, to_file='model.png', show_shapes=True, dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCeDsDZVVeb8",
        "outputId": "57bfb5ac-094b-493a-ed3f-1ee89626b55d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRHR-ypHVeb8"
      },
      "source": [
        "In total, this model has 550 trainable parameters. A rule of thumb in order to ensure generalization is to have 10 times more examples than the number of parameters, so this is consistent with the data budget we have. This can also be evaluated by looking at the learning curves and comparing the validation and training errors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2aEwPWFVeb9"
      },
      "source": [
        "## Training the network\n",
        "Now we set the optimizer options and compile the model. The categorical crossentropy is used as a loss function and the accuracy is selected as a metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE7MfiV_Veb9"
      },
      "outputs": [],
      "source": [
        "adam = optimizers.Adam(1e-3)\n",
        "model.compile(loss={'flow_pattern':'categorical_crossentropy',\n",
        "                    },\n",
        "              optimizer=adam,\n",
        "              metrics={'flow_pattern':['accuracy']\n",
        "                       },\n",
        "             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcnxGUx8Veb9",
        "outputId": "07adf696-e35b-4fa8-c83b-f8aff2a470f0"
      },
      "outputs": [],
      "source": [
        "X_train.shape, Y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oelm4fneVeb9"
      },
      "source": [
        "The model is fitted to the data using the `model.fit` method. After training the model is saved and the history during training is kept in `history`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "id": "FiFY57DFVeb9",
        "outputId": "e208ead7-6b4b-42a9-fd16-2f4d0d637e68",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "history = model.fit(x=X_train, y=Y_train, verbose=0, batch_size=n_batch, epochs=n_epochs,\n",
        "                    validation_data=(X_val, Y_val), callbacks=[PrintEveryNEpochs(10)])\n",
        "model.save(fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k66HytAXeZ1l"
      },
      "source": [
        "## Optimizing hyperparameters\n",
        "\n",
        "The `history` object is used to plot the learning curve of our model. It is seen that the validation error is still decreasing with the training error, however the training error is stagnant (although it is still decreasing slowly). This indicates that either too much regularization is being used or the model is too simple. We can improve this by either decreasing the regularization (in this case, the dropout rate) or increasing the model complexity (by increasing the number of layers or neurons). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPLgMbZUeZ1l",
        "outputId": "c7b74bf0-7568-4077-87ad-d9b64137c3ed",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.loglog(history.history['loss'], label='Training Error')\n",
        "ax.loglog(history.history['val_loss'], label='Validation Error')\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Flow pattern categorical crossentropy loss\")\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(history.history['accuracy'], label='Training Error')\n",
        "ax.plot(history.history['val_accuracy'], label='Validation Error')\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Flow pattern accuracy\")\n",
        "ax.set_ylim([0, 1])\n",
        "ax.legend()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHR2kw03eZ1m"
      },
      "source": [
        "The curve below is for the same model but with no dropout. It is seen that after $\\approx 3x10^3$, the model starts overfitting and further training will actually result in worse results. However, it is seen that the model accuracy is higher than the previous one with dropout rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1LdfOk-eZ1m",
        "outputId": "37f41d42-ad95-4efd-ec49-775c41976c81",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.loglog(history.history['loss'], label='Training Error')\n",
        "ax.loglog(history.history['val_loss'], label='Validation Error')\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Flow pattern categorical crossentropy loss\")\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(history.history['accuracy'], label='Training Error')\n",
        "ax.plot(history.history['val_accuracy'], label='Validation Error')\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.set_ylabel(\"Flow pattern accuracy\")\n",
        "ax.set_ylim([0, 1])\n",
        "ax.legend()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVVqNpbqeZ1m"
      },
      "source": [
        "This is an iterative process and the hyperparameter tuning (i.e. dropout rates, hidden layer units, etc.) can be tuned using different approaches. Let's assume that we already did this hyperparameter optimization and let's explore the `model` capabilities. First, the model is loaded from `fname` using `load_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwzRva_weZ1m"
      },
      "outputs": [],
      "source": [
        "model = load_model(fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RR2ul9lVeb_"
      },
      "source": [
        "## Using the model\n",
        "\n",
        "The two functions below take a model, some input parameters and create a flow pattern map and flow probability as a function of the superficial velocities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jswztk1UVeb_"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "import seaborn as sns\n",
        "\n",
        "fp_dict = {\n",
        "    1:'Dispersed Bubble',\n",
        "    2:'Stratified Smoth',\n",
        "    3:'Stratified Wavy',\n",
        "    4:'Annular',\n",
        "    5:'Intermitent',\n",
        "    0:'Bubble Flow',\n",
        "}\n",
        "\n",
        "def plot_flow_pattern(model, rho_l=1000., rho_g=1.8, mu_l=1e-3, mu_g=0.000015, sigma=0.070, D=0.051, angle=0):\n",
        "        sin_angle = np.sin(angle*np.pi/180)\n",
        "        cos_angle = np.cos(angle*np.pi/180)\n",
        "        vsls = np.logspace(-2, 1, 300)\n",
        "        vsgs = np.logspace(-2, 2, 300)\n",
        "        vsls, vsgs = np.meshgrid(vsls, vsgs)\n",
        "        X = np.array([\n",
        "                [rho_l, rho_g, mu_l, mu_g, sigma, D, sin_angle, cos_angle, vsl, vsg]\n",
        "                for (vsl, vsg) in zip(vsls.flatten(), vsgs.flatten())]\n",
        "            )\n",
        "        X = scaler.transform(X)\n",
        "        flow_patterns = model.predict(X).argmax(axis=1)\n",
        "        \n",
        "        fig, ax = plt.subplots()\n",
        "        cmap = ListedColormap(sns.color_palette('Paired', n_colors=6).as_hex())\n",
        "        ax.contourf(vsgs, vsls, flow_patterns.reshape(vsgs.shape), cmap=cmap, vmin=0, vmax=5)\n",
        "        ax.set_xscale('log')\n",
        "        ax.set_yscale('log')\n",
        "        ax.set_xlabel('$v_{sg}$ [m/s]')\n",
        "        ax.set_ylabel('$v_{sl}$ [m/s]')\n",
        "        plt.tight_layout()\n",
        "\n",
        "def plot_flow_probability(model, cmap='RdBu', rho_l=1000., rho_g=1.8, mu_l=1e-3, mu_g=0.000015, sigma=0.070, D=0.051, angle=0):\n",
        "        sin_angle = np.sin(angle*np.pi/180)\n",
        "        cos_angle = np.cos(angle*np.pi/180)\n",
        "        vsls = np.logspace(-2, 1, 300)\n",
        "        vsgs = np.logspace(-2, 2, 300)\n",
        "        vsls, vsgs = np.meshgrid(vsls, vsgs)\n",
        "        X = np.array([\n",
        "                [rho_l, rho_g, mu_l, mu_g, sigma, D, sin_angle, cos_angle, vsl, vsg]\n",
        "                for (vsl, vsg) in zip(vsls.flatten(), vsgs.flatten())]\n",
        "            )\n",
        "        X = scaler.transform(X)\n",
        "        flow_patterns = model.predict(X)\n",
        "        fig, axes = plt.subplots(3,2, sharex=True, sharey=True, figsize=(10,10))\n",
        "        for pattern_code, ax in enumerate(axes.flatten()):\n",
        "            im = ax.pcolormesh(vsgs, vsls, flow_patterns[:, pattern_code].reshape(vsgs.shape),\n",
        "                               vmin=0, vmax=1,\n",
        "                               cmap=cmap)\n",
        "            plt.colorbar(im, ax=ax)\n",
        "            ax.set_xscale('log')\n",
        "            ax.set_yscale('log')\n",
        "            ax.set_xlabel('$v_{sg}$ [m/s]')\n",
        "            ax.set_ylabel('$v_{sl}$ [m/s]')\n",
        "            ax.set_title(fp_dict[pattern_code])\n",
        "            plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfnHZVc-VecA"
      },
      "source": [
        "We can use this functions to plot the flow pattern map for different conditions. This is done below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gQqZNjkvVecA",
        "outputId": "f9dacb71-df23-488d-86ed-113c82202aaa",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "plot_flow_pattern(model, angle=0)\n",
        "plot_flow_probability(model, angle=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbg6545sVecA"
      },
      "source": [
        "Notice that the probability of being stratified smooth is actually not that high in the stratified region. However, it is higher than other flow patterns, so the predicted flow pattern is stratified smooth, although with a lower confidence. This already illustrates one of the benefits of using this type of model. It also gives some kind of confidence values. Let's check the flow pattern for vertical case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZKVnS4BBVecA",
        "outputId": "eddc99ba-0ad6-43a4-9509-5db45e0d67e6",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "plot_flow_pattern(model, angle=90)\n",
        "plot_flow_probability(model, angle=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMk2A88YVecB"
      },
      "source": [
        "It is seen that in the vertical case the model predicts intermittent flow, even though it should predict bubbly flow in the region of low `v_sl` and `v_sg`. Can you think of a reason why this is the case?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6-zRN3TVecF"
      },
      "source": [
        "Let's check the number of points for each flow pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRCrEW4SVecF",
        "outputId": "702b5b18-3ce5-426a-d9ca-6ee1cb1941b3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df_train.flow_pattern_code.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG5gmZL7VecF",
        "outputId": "1fc6eb8d-ceb0-406d-f91d-fbafc8ffeac7"
      },
      "outputs": [],
      "source": [
        "df_train.flow_pattern_code.value_counts()/len(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_t8kQ7eWVecF",
        "outputId": "3a254fbb-4bde-4178-ee5b-7f64968f20e5"
      },
      "outputs": [],
      "source": [
        "df_val.flow_pattern_code.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD0JOjZTVecG",
        "outputId": "0f199523-25e7-447b-8105-92d126d23adf",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df_val.flow_pattern_code.value_counts()/len(df_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBJ5Iig2VecG"
      },
      "source": [
        "The number of points with intermittent flow pattern represents almost $52\\%$ of the dataset, while points with bubbly flow pattern are only $3.7\\%$. When we were using the categorical crossentropy loss, it is implicit that the number of points in each class is balanced, i.e. they have the same fraction of points. However, in our data the number of points with bubbly flow are underrepresented and it was easier for the model to just predict intermittent flow than to try and predict bubbly flow. This is more clearly seen if we check the confusion matrix of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcjDPZhsVecG"
      },
      "source": [
        "## Evaluating the model\n",
        "\n",
        "The confusion matrix shows how a given class (in our case, a flow pattern) is predicted by the model. The rows indicate the real value while the columns represent the predicted value. Hence, we see that $99.6\\%$ of the points which are Bubble flow are predicted as Intermittent flow. Notice that no points are predicted as bubble flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLgzr9MKVecG",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "def calculate_confusion_matrix(df_):\n",
        "    X = scaler.transform(df_[inputs].values)\n",
        "    predicted_fp = model.predict(X).argmax(axis=1)\n",
        "\n",
        "    fps = [fp_dict[c] for c in range(6)]\n",
        "    df_results = pd.DataFrame(confusion_matrix(df_['flow_pattern_code'], predicted_fp),\n",
        "                              columns=fps, index=fps)\n",
        "    return (df_results.T/df_results.sum(axis=1)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "u8Kx6sj5VecG",
        "outputId": "b6c19f1e-71f2-4f15-e557-b85d1995654f"
      },
      "outputs": [],
      "source": [
        "calculate_confusion_matrix(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "ciaBjweVVecG",
        "outputId": "57f7a52a-d131-4b62-ba4c-22c485dbdf4d"
      },
      "outputs": [],
      "source": [
        "calculate_confusion_matrix(df_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "futuOvmZVecH",
        "outputId": "d84caecf-94fd-4cd2-da45-cb23189938c8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "calculate_confusion_matrix(df_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWNJndl5VecH"
      },
      "source": [
        "As a comparison with the Barnea model, the confusion matrix for the Barnea flow pattern is given below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "_XcrV0JZVecH",
        "outputId": "b838a754-a584-4987-cf40-5644f31a2449",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "fps = [fp_dict[c] for c in range(6)]\n",
        "df_results_barnea = pd.DataFrame(confusion_matrix(df['flow_pattern_code'], df['barnea_flow_pattern_code']),\n",
        "                          columns=fps, index=fps)\n",
        "(df_results_barnea.T/df_results_barnea.sum(axis=1)).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdkklw0NVecH"
      },
      "source": [
        "It is seen that Barnea is a better predictor for bubble flow. However, it gives worse results for the other flow patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-DEdAMfVecH"
      },
      "source": [
        "# Example\n",
        "\n",
        "A given line transport 500x10$^3$ BPD (@ P,T) and 5.0  MM ft$^3$/day (@ P,T), The fluid properties are presented below.  To solve Flow oscillation problems caused by slugs, a finger type slug catcher of one bottle is proposed at the end of the line. \n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The idea is to stratify the phases and extract the gas at the top of the line.  The current line is horizontal and 26 in. ID. For this condition answer the following questions:\n",
        " \n",
        "|     Liquid Density     |     Liquid Viscosity    |     Surface Tension    |     Gas Density     |     Gas Viscosity    |\n",
        "|:----------------------:|:-----------------------:|:----------------------:|:-------------------:|:--------------------:|\n",
        "|         51         |          3         |         32        |        5           |       1E-02     |\n",
        "|        [Lb/ft$^3$]     |           [cp]          |        [Dyna/cm]       |       [Lb/ft$^3$]   |          [cp]        |\n",
        "\n",
        "\n",
        "Part 1:\n",
        " What is the pipe diameter that will stratify the phases in horizontal flow, consider: 26, 30, 36, 48 and  60 in.\n",
        " \n",
        "Part 2:\n",
        " What is the inclination angle that can stratified the phases for the 26 in. ID line.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgmmYWtyVecI"
      },
      "source": [
        "## Problem inputs and preprocessing\n",
        "\n",
        "The input data is given below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsVR9BfAVecI"
      },
      "outputs": [],
      "source": [
        "q_l_sc_field = 500e3 # stb/Day\n",
        "q_g_sc_field = 5e6 # ft^3/Day\n",
        "rho_l_field = 51 # lb/ft3\n",
        "mu_l_field = 3 # cp\n",
        "sigma_field = 32 # Dyne/cm\n",
        "rho_g_field = 5 # lb/ft3\n",
        "mu_g_field = 1e-2 # cp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjGfY5BoVecI"
      },
      "source": [
        "Conversion coefficients:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LSvlVsVVecI"
      },
      "outputs": [],
      "source": [
        "lb_to_kg = 0.453592\n",
        "ft_to_m = 12*0.0254\n",
        "stb_to_ft3 = 42*231/12**3\n",
        "days_to_sec = 24*3600\n",
        "dyne_cm_to_N_m = 1e-3\n",
        "cp_to_Pa_s = 1e-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGmKwrvKVecI"
      },
      "source": [
        "Convert to SI units:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSfORYfbVecI"
      },
      "outputs": [],
      "source": [
        "rho_l = rho_l_field*lb_to_kg/ft_to_m**3\n",
        "rho_g = rho_g_field*lb_to_kg/ft_to_m**3\n",
        "sigma = dyne_cm_to_N_m*sigma_field\n",
        "mu_l = cp_to_Pa_s*mu_l_field\n",
        "mu_g = cp_to_Pa_s*mu_g_field\n",
        "q_l_sc = q_l_sc_field*stb_to_ft3*ft_to_m**3/days_to_sec\n",
        "q_g_sc = q_g_sc_field*ft_to_m**3/days_to_sec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCOJD1e-VecJ"
      },
      "source": [
        "## Part 1 - Selection of $D$\n",
        "Create a grid of values for $D$ and $\\theta$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRaWqtYoVecJ"
      },
      "outputs": [],
      "source": [
        "Ds = np.linspace(26, 60, 100)*0.0254\n",
        "angles = np.linspace(-45, 45, 91)\n",
        "Ds, angles = np.meshgrid(Ds, angles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOq9-JFHVecJ"
      },
      "source": [
        "Create the array with corresponding inputs, calculating $\\sin \\theta$, $\\cos \\theta$, $v_{sl}=q_{l,sc}/A$ and $v_{sg}=q_{g,sc}/A$. Normalize using `scaler.transform` (this is the same `scaler` which was used in training). Predict using the `model.predict` method. The probabilities for stratified smooth and stratified wavy are given by the columns 3 and 4 of `fp` (remember that python indexing starts at 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLdFCMH_VecJ"
      },
      "outputs": [],
      "source": [
        "X = np.array([\n",
        "                [rho_l, rho_g, mu_l, mu_g, sigma, D, np.sin(angle*np.pi/180), np.cos(angle*np.pi/180),\n",
        "                 q_l_sc/(np.pi*D**2/4), q_g_sc/(np.pi*D**2/4)]\n",
        "                for (D, angle) in zip(Ds.flatten(), angles.flatten())]\n",
        "            )\n",
        "X = scaler.transform(X)\n",
        "fp = model.predict(X)\n",
        "stratified = fp[:, 2] + fp[:, 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqAu-mnHVecJ"
      },
      "source": [
        "Plot the probability of being stratified as a function of $D$ and $\\theta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "_tPxQv7rVecJ",
        "outputId": "921b4e48-f382-44ef-82ea-c0388b2691e2"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, sharex=True, sharey=True, figsize=(6, 4.5))\n",
        "im = ax.pcolormesh(Ds/0.0254, angles, stratified.reshape(Ds.shape),\n",
        "                   vmin=0, vmax=1,\n",
        "                   cmap='RdBu')\n",
        "plt.colorbar(im, ax=ax)\n",
        "ax.set_xlabel('$D$ [in]')\n",
        "ax.set_ylabel('$\\\\theta$ [°]')\n",
        "ax.set_title('Probability of Stratified flow')\n",
        "ax.set_xticks([26, 30, 36, 48, 60])\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NzdsE14VecK"
      },
      "source": [
        "We can also use slices of the previous plot to get probability of stratified flow for constant $\\theta$ or $D$. Probability of stratified flow for a constant angle is given below. You can change the index `i` and check how the probability curve changes for different inclinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "THGrDlA2VecK",
        "outputId": "7994f9ec-8df7-4ff5-93d6-0dbf6bd3d478"
      },
      "outputs": [],
      "source": [
        "i = 45\n",
        "fig, ax = plt.subplots()\n",
        "stratified = stratified.reshape(Ds.shape)\n",
        "ax.plot(Ds[i]/0.0254, stratified[i])\n",
        "ax.set_xticks([26, 30, 36, 48, 60])\n",
        "ax.set_title(f'Probability of Stratified flow for {angles[i].mean()}°')\n",
        "ax.set_ylim([0, 1])\n",
        "ax.set_xlabel('D [in]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq_SLnQnVecK"
      },
      "source": [
        "## Part 2 - Selection of $\\theta$\n",
        "\n",
        "Similarly, we can slice the data in a different axis and get the behavior of the stratified probability for a constant diameter. The index `i` can also be changed to show different diameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "kPAtGkO0VecK",
        "outputId": "218546ac-6621-4686-ca51-e0a2832e90bf"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(angles.T[i], stratified.T[i])\n",
        "ax.set_title(f'Probability of Stratified flow for {Ds.T[i].mean()/0.0254:.3} in')\n",
        "ax.set_ylim([0, 1])\n",
        "ax.set_xlabel('$\\\\theta$ [°]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "9p8si3rJVecK",
        "outputId": "df454c28-fa5a-48a1-db89-cce69a2d1bb9"
      },
      "outputs": [],
      "source": [
        "i = 64\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(angles.T[i], stratified.T[i])\n",
        "ax.set_title(f'Probability of Stratified flow for {Ds.T[i].mean()/0.0254:.3} in')\n",
        "ax.set_ylim([0, 1])\n",
        "ax.set_xlabel('$\\\\theta$ [°]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "f7MZntqVVecL",
        "outputId": "55859397-60f6-410b-d240-34551c3e3c30",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "i = 99\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(angles.T[i], stratified.T[i])\n",
        "ax.set_title(f'Probability of Stratified flow for {Ds.T[i].mean()/0.0254:.3} in')\n",
        "ax.set_ylim([0, 1])\n",
        "ax.set_xlabel('$\\\\theta$ [°]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BPJY-PiVecL"
      },
      "source": [
        " - Try to answer the questions of the example based on the plots above.\n",
        " - Is it risky to select a 26 in diameter with a downward inclination? \n",
        " - How would your answer change if the flow conditions of interest were significantly different than the ones present in the database?\n",
        " - Try to plot an operation envelope ($v_{sl}$ vs. $v_{sg}$) where stratified flow is likely to occur (this is similar to what was done in the example for $\\theta$ vs. $D$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEp_eUlsVecL"
      },
      "source": [
        "# Questions?\n",
        "\n",
        "If you have any questions, feel free to send me an e-mail at `vik8290@utulsa.edu`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Up6UwByfVeb5",
        "oqfJtq76Veb7",
        "_2aEwPWFVeb9",
        "k66HytAXeZ1l",
        "4RR2ul9lVeb_",
        "dcjDPZhsVecG",
        "YgmmYWtyVecI",
        "bCOJD1e-VecJ",
        "mq_SLnQnVecK"
      ],
      "name": "7 Flow Pattern for Slug Catcher Design.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
