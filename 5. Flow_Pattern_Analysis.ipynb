{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "quantitative-rally",
      "metadata": {
        "id": "quantitative-rally"
      },
      "source": [
        "# Comparison of Machine Learning methods performance to predict the flowpattern in a nearly horizontal pipeline\n",
        "### By: Gerardo Vera\n",
        "\n",
        "M.E. Petroleum Engineer\\\n",
        "email: gev7313@utulsa.edu - gerryac568@gmail.com\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "This project shows a way to classify the flow through a nearly horizontal pipeline, more properly said, between minus and plus ten degrees of inclination. The study will only cover the data recorded using a two inch pipeline. The categories for the classification are five, Dispersed Bubble, Stratified Smooth, Stratified Wavy, Annular and Intermittent flow.\n",
        "\n",
        "The method for the classification will be machine learning, in this case, a comparison of four methods, the decision tree algorithm, the random forest algorithm, the naive Bayes classifier and the support vector machine techniques.\n",
        "\n",
        "To demonstrate the degree of accuracy of the methods, a percentage of the data recorded will be used as feed, to train the models, and the rest will be used to test the models results. The data selected will always be selected randomly, so the accuracy may vary amongst several runs of the program, but it shouldn't vary too much.\n",
        "\n",
        "### Configuration\n",
        "\n",
        "We start importing the libraries we will use in the study. Numpy is a mathematical library that helps us dealing with numbers and operations. Pandas is a dataframe manager which will help us to import and manipulate the dataset to fit our needs. The sklearn library is focused towards predictive data analysis and machine learning functions, from this one we will import several functions.\n",
        "\n",
        "From sklearn.tree we import the DecisionTreeClassifier function which is the decision tree itself. We do the same with RandomForestClassifier from sklearn.ensemble. GaussianNB from sklearn.naive_bayes implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian. Confusion_matrix will help us to create automatically confusion matrices and accuracy_score will help us to evaluate the models. And finally we import svm which is the support vector machine.\n",
        "\n",
        "Matplotlib is the library that will help us to display our results in a more user friendly way (using graphs), seaborn is another library dedicated to plotting data and information and finally, the immediately below line is to display these graphs in the same document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65568ad0",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sunrise-belle",
      "metadata": {
        "id": "sunrise-belle"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#Decision Tree Algorithm\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "#Random Forest Algorithm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#Naive Bayes Classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "#Support Vector Machine\n",
        "from sklearn import svm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "closed-syndication",
      "metadata": {
        "id": "closed-syndication"
      },
      "source": [
        "### Data Reading and Cleaning\n",
        "\n",
        "Now that the configuration is done, we can begin reading the data. First, we read the data directly from the excel file, to a new variable called data1, using the pandas's library command read_excel. We use the keywords \"header\" and \"skiprows\" to specify that this is going to be a multiindex dataframe since there are three lines in the excel file with titles and subtitles and to skip the fourth line because it contains the units which are not important for our study.\n",
        "\n",
        "To start filtering the data rows and columns, we declare a new variable, data2, and we put in it some rows and columns from data1, for this we use the comand iloc, proper of the dataframes. We put all the rows below the row 3170, we got this number by checking the excel file and checking which data points we want to work with. For the columns we use their names to select them, so we specify a level and take the columns in those levels with the names we are specifying.\n",
        "\n",
        "To further filter the data, we now use the inner diameter column because we are interested in working with the datapoints recorded using a diameter bigger than 0.05 meters which is a little less than two inches. We set only a lower limit because the higher value in this column is precisely two inches.\n",
        "\n",
        "Finally, the last filter. We position ourselves on the column \"Angle\", under the subtitle \"System Geometry\" and under the title \"Input Data\". Now we extract to a new variable, called data4, the rows which inclination is bigger or equal to -10 and smaller or equal to 10 degrees.\n",
        "\n",
        "We can now take a look at the five first rows of our new dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imperial-telling",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "imperial-telling",
        "outputId": "bc99d99c-5fac-42d8-82db-a22398f75d98"
      },
      "outputs": [],
      "source": [
        "data1 = pd.read_excel(\"/content/drive/MyDrive/ALCE/DataBasePereyraetal2012.xls\", \n",
        "                      header = [0, 1, 2], skiprows = [3])\n",
        "data2 = data1.iloc[data1.index[data1['Input Data', 'Test Number', 'Test Code'].str.contains('Shoham')], \n",
        "                   (data1.columns.get_level_values(2)=='Test Code') | \n",
        "                   (data1.columns.get_level_values(2)=='P') | \n",
        "                   (data1.columns.get_level_values(2)=='Type of liquid') | \n",
        "                   (data1.columns.get_level_values(2)=='Type of Gas') | \n",
        "                   (data1.columns.get_level_values(2)=='DenL') | (data1.columns.get_level_values(2)=='DenG') | \n",
        "                   (data1.columns.get_level_values(2)=='VisL') | (data1.columns.get_level_values(2)=='VisG') | \n",
        "                   (data1.columns.get_level_values(2)=='ST') | (data1.columns.get_level_values(2)=='ID') | \n",
        "                   (data1.columns.get_level_values(2)=='Ang') | (data1.columns.get_level_values(2)=='Vsl') | \n",
        "                   (data1.columns.get_level_values(2)=='Vsg') | \n",
        "                   (data1.columns.get_level_values(1)=='Unnamed: 17_level_1') | \n",
        "                   (data1.columns.get_level_values(1)=='Unnamed: 18_level_1')]\n",
        "data3 = data2[(data2[('Input Data','System Geometry','ID')] > 0.05) & \n",
        "              (data2[('Input Data','System Geometry','Ang')] >= -10) & \n",
        "              (data2[('Input Data','System Geometry','Ang')] <= 10)]\n",
        "data4 = data3.reset_index(drop = True)\n",
        "data4.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vocational-scanning",
      "metadata": {
        "id": "vocational-scanning"
      },
      "source": [
        "We now define a new variable, called categories, and store in it the abbreviations of the flowpatterns.\n",
        "\n",
        "To split our data into training and testing data, we create a new column called \"is_train\", in our dataset, and we fill it with boolean data, setting randomly 75 percent of it with the value True and the rest with the value False.\n",
        "\n",
        "We create two new dataframes, called train and test, and we fill them with the data from our data4 dataframe, depending on the value of the previously created column \"is_train\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "athletic-marina",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "athletic-marina",
        "outputId": "e42d6391-2895-40bb-f125-b4527e1e4f31"
      },
      "outputs": [],
      "source": [
        "categories = ['A', 'DB', 'I', 'SS', 'SW']\n",
        "np.random.seed(185)\n",
        "data4['is_train'] = np.random.uniform(0, 1, len(data4)) <= .75\n",
        "train, test = data4[data4['is_train'] == True], data4[data4['is_train'] == False]\n",
        "data4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "useful-salon",
      "metadata": {
        "id": "useful-salon"
      },
      "source": [
        "In order to use the naive bayes classifier we need to further split our data, each dataset, train and test, into two new datasets, inputs and outputs. We do this by selecting the different columns of each dataframe. So in the new data2 dataframe, we have the training inputs, in the new data3 dataframe, we have the training outputs, in the new data5 dataframe, we store the testing inputs and in the data6 dataframe, we store the testing outputs.\n",
        "\n",
        "Finally, we transform the training output dataframe into an array, and then we reshape it into a one dimension vector because that is the input format the function model.fit is expecting. Otherwise we would get a warning message, however, the program would still work as expected.\n",
        "\n",
        "To check the transformation we display the data stored in the variable data3, but only the first 20 datapoints so it does not take too much space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "awful-swaziland",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awful-swaziland",
        "outputId": "8cc8e11f-3aea-42f3-ea3f-ed87c876a92f"
      },
      "outputs": [],
      "source": [
        "data2 = train.iloc[:, (train.columns.get_level_values(2)=='Vsl') | (train.columns.get_level_values(2)=='Vsg') | (train.columns.get_level_values(2)=='Ang')]\n",
        "data3 = train.iloc[:, (train.columns.get_level_values(1)=='Unnamed: 17_level_1')]\n",
        "data5 = test.iloc[:, (test.columns.get_level_values(2)=='Vsl') | (test.columns.get_level_values(2)=='Vsg') | (test.columns.get_level_values(2)=='Ang')]\n",
        "data6 = test.iloc[:, (test.columns.get_level_values(1)=='Unnamed: 17_level_1')]\n",
        "data3 = np.asarray(data3)\n",
        "data3 = data3.reshape(-1, )\n",
        "data3[0:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "described-inspection",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "described-inspection",
        "outputId": "e36cdf43-83f6-4e6f-c0aa-9a7889123044"
      },
      "outputs": [],
      "source": [
        "y, label = pd.factorize(train['Output', 'Unnamed: 17_level_1', 'Flow Pattern'])\n",
        "ytest, label = pd.factorize(test['Output', 'Unnamed: 17_level_1', 'Flow Pattern'])\n",
        "y[0:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "possible-pocket",
      "metadata": {
        "id": "possible-pocket"
      },
      "source": [
        "### Fitting the models\n",
        "\n",
        "Now that we have input data cleaned, we can train our naive Bayes model. First we create a gaussian model, we use the gaussian naive Bayes method because is the best suited for dealing with numeric inputs. Then we train it with our training input and output datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vertical-handy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vertical-handy",
        "outputId": "817111ba-b054-427c-be6d-545435f96a3c"
      },
      "outputs": [],
      "source": [
        "#Decision Tree Algorithm\n",
        "modelDTA = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100)\n",
        "modelDTA.fit(data2, data3)\n",
        "#Random Forest Algorithm\n",
        "modelRFA = RandomForestClassifier(n_jobs = 2, random_state = 0)\n",
        "modelRFA.fit(data2, data3)\n",
        "#Naive Bayes Classifier\n",
        "modelNBC = GaussianNB()\n",
        "modelNBC.fit(data2, data3)\n",
        "#Support Vector Machine\n",
        "modelSVM = svm.SVC(C = 100)\n",
        "modelSVM.fit(data2, data3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "addressed-seven",
      "metadata": {
        "id": "addressed-seven"
      },
      "source": [
        "We apply the model to the testing inputs and store the results in a variable called predict, we should compare this results with the data stored in the dataframe data6 (output data recorded from experiments) to check the accuracy of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "danish-devices",
      "metadata": {
        "id": "danish-devices"
      },
      "outputs": [],
      "source": [
        "#Decision Tree Algorithm\n",
        "predictDTA = modelDTA.predict(data5)\n",
        "#Random Forest Algorithm\n",
        "predictRFA = modelRFA.predict(data5)\n",
        "#Naive Bayes Classifier\n",
        "predictNBC = modelNBC.predict(data5)\n",
        "#Support Vector Machine\n",
        "predictSVM = modelSVM.predict(data5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alive-feeling",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alive-feeling",
        "outputId": "f91a5208-b3be-4c21-8b63-962161d7bb61"
      },
      "outputs": [],
      "source": [
        "#Decision Tree Algorithm\n",
        "print(predictDTA[0:25])\n",
        "print(\"\")\n",
        "#Random Forest Algorithm\n",
        "print(predictRFA[0:25])\n",
        "print(\"\")\n",
        "#Naive Bayes Classifier\n",
        "print(predictNBC[0:25])\n",
        "print(\"\")\n",
        "#Support Vector Machine\n",
        "print(predictSVM[0:25])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "capable-piano",
      "metadata": {
        "id": "capable-piano"
      },
      "source": [
        "### Analysing results\n",
        "\n",
        "To check our results, we create a confusion matrix, comparing the experimental with the predicted data.\n",
        "\n",
        "To display the result in a more appealing way, we show the confusion matrix as a heatmap, to appreciate the right and wrong predictions by colors.\n",
        "\n",
        "With the keyword \"font_scale\" we set the font size of the plot, then with the keyword \"figsize\" we define the size of the plot. Then we draw the heatmap, using as reference the confusion matrix created before. We set the parameter \"square\" as true, to show each box of the confusion matrix as a perfect square, \"annot\" as true to show the flow pattern abbreviation for each row and column, \"fmt\" as 'd' to show the numbers in decimal format, \"cbar\" as true to show the color reference and then we match the rows and columns with a variable containing the abbreviations. At last we add the axis labels to show which one is the predicted and which one is the experimental data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "occupational-spank",
      "metadata": {
        "id": "occupational-spank"
      },
      "source": [
        "Another way of checking our results is by using the accuracy_score function, which will show us the percentage of correctly predicted flow patterns. This is like dividing the accurate predictions over the wrong prediction in the confusion matrix and multiplying it by a hundred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "understood-safety",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "understood-safety",
        "outputId": "589a6edf-8784-41bb-a389-1ec2c36e5dbc",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "title = ['Decision Tree Algorithm Resulst:', 'Random Forest Algorithm Results:',\n",
        "         'Naive Bayes Classifier Results:', 'Support Vector Machine Results:']\n",
        "accuracy = [predictDTA, predictRFA, predictNBC, predictSVM]\n",
        "for x, y in zip(title, accuracy):\n",
        "    title1 = '\\033[1m' + x + '\\033[0m'\n",
        "    z = title1.center(73)\n",
        "    print('')\n",
        "    print(z)\n",
        "    print('')\n",
        "    print('Accuracy:', accuracy_score(data6, y))\n",
        "    sns.set(font_scale=1.6)\n",
        "    mat = confusion_matrix(data6, y)\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    sns.heatmap(mat.T, square = True, annot = True, fmt = 'd', cbar = True,\n",
        "               xticklabels = categories,\n",
        "               yticklabels = categories,\n",
        "               vmax=150,\n",
        "               ax = ax)\n",
        "    plt.title(x[0:23])\n",
        "    plt.xlabel('True label', size = 20)\n",
        "    plt.ylabel('Predicted label', size = 20);\n",
        "    plt.show()\n",
        "    print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caroline-charger",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caroline-charger",
        "outputId": "fbeb89ad-2192-4545-a360-e9115fb515ab"
      },
      "outputs": [],
      "source": [
        "title = ['Decision Tree Algorithm Resulst:', 'Random Forest Algorithm Results:',\n",
        "         'Naive Bayes Classifier Results:', 'Support Vector Machine Results:']\n",
        "accuracy = [predictDTA, predictRFA, predictNBC, predictSVM]\n",
        "for x, y in zip(title, accuracy):\n",
        "    str = '\\033[1m' + x + '\\033[0m'\n",
        "    print('')\n",
        "    print(str)\n",
        "    print('')\n",
        "    print('Accuracy:', \"%.2f\" % (accuracy_score(data6, y)* 100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "4 Flow Pattern Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
