{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3047821e",
   "metadata": {},
   "source": [
    "# Downhole Pressure Prediction for ESP Wells\n",
    "This notebook uses machine learning to predict downhole pressure (PDP_psi) for ESP wells based on production and operational data. It is designed for classroom exercises and is compatible with Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138cae64",
   "metadata": {},
   "source": [
    "## 📋 Google Colab Setup Instructions\n",
    "\n",
    "### 🔧 What's Pre-installed in Colab?\n",
    "Google Colab comes with most common packages:\n",
    "- ✅ **pandas, numpy, matplotlib, seaborn, scikit-learn** (always available)\n",
    "- ✅ **tensorflow** (usually pre-installed)\n",
    "- ❓ **xgboost, lightgbm** (may need installation)\n",
    "\n",
    "### 📝 Setup Steps:\n",
    "\n",
    "1. **Install Missing Packages**: Run the package installation cell below\n",
    "2. **Load Your Data**: Choose one of these methods:\n",
    "   - 📤 **Upload directly** (easiest for small files)\n",
    "   - 📂 **Google Drive** (best for repeated use)\n",
    "   - 🌐 **Public URL** (if data is hosted online)\n",
    "\n",
    "3. **Enable GPU** (optional but recommended):\n",
    "   - Runtime → Change runtime type → Hardware accelerator → GPU\n",
    "\n",
    "### 💡 Pro Tips:\n",
    "- Upload files < 25MB directly\n",
    "- Use Google Drive for larger files or repeated access\n",
    "- Save your work to Drive: File → Save a copy in Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e842f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "import joblib  # for model serialization\n",
    "from tensorflow.keras.models import Sequential  # for ANN\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "import lightgbm as lgb\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading - Multiple options for different environments\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"🌐 Google Colab detected\")\n",
    "    \n",
    "    # Method 1: Upload file directly\n",
    "    print(\"\\n📁 Option 1: Upload CSV file directly\")\n",
    "    print(\"Click 'Choose Files' below to upload ESP_PDHG_WT.csv:\")\n",
    "    \n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    if uploaded:\n",
    "        data_path = list(uploaded.keys())[0]\n",
    "        print(f\"✅ File uploaded: {data_path}\")\n",
    "    else:\n",
    "        print(\"❌ No file uploaded. Please try again or use alternative method.\")\n",
    "        data_path = None\n",
    "        \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"💻 Local environment detected\")\n",
    "    data_path = 'ESP_PDHG_WT.csv'\n",
    "\n",
    "# Load the dataset\n",
    "if data_path:\n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"✅ Dataset loaded successfully! Shape: {df.shape}\")\n",
    "        print(f\"📊 Columns: {list(df.columns)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ File not found.\")\n",
    "        if IN_COLAB:\n",
    "            print(\"💡 Try uploading the file using the cell above\")\n",
    "        else:\n",
    "            print(\"💡 Ensure ESP_PDHG_WT.csv is in the same directory as this notebook\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading file: {e}\")\n",
    "\n",
    "# Preview data\n",
    "if 'df' in locals():\n",
    "    df.head()\n",
    "else:\n",
    "    print(\"⚠️ Dataset not loaded. Please resolve the data loading issue above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde7d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Data Loading Methods (run only if upload method above failed)\n",
    "\n",
    "# Method 2: Load from Google Drive\n",
    "def load_from_drive():\n",
    "    \"\"\"Mount Google Drive and load dataset\"\"\"\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        print(\"📂 Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "        print(\"✅ Google Drive mounted successfully!\")\n",
    "        \n",
    "        # List common paths to help users find their file\n",
    "        import os\n",
    "        print(\"\\n📁 Common Drive locations:\")\n",
    "        print(\"- /content/drive/MyDrive/\")\n",
    "        print(\"- /content/drive/MyDrive/Colab Notebooks/\")\n",
    "        print(\"- /content/drive/MyDrive/Data/\")\n",
    "        \n",
    "        # User needs to update this path\n",
    "        drive_path = '/content/drive/MyDrive/ESP_PDHG_WT.csv'\n",
    "        \n",
    "        if os.path.exists(drive_path):\n",
    "            df_drive = pd.read_csv(drive_path)\n",
    "            print(f\"✅ Dataset loaded from Drive! Shape: {df_drive.shape}\")\n",
    "            return df_drive\n",
    "        else:\n",
    "            print(f\"❌ File not found at: {drive_path}\")\n",
    "            print(\"💡 Update the 'drive_path' variable with your file location\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error accessing Google Drive: {e}\")\n",
    "        return None\n",
    "\n",
    "# Method 3: Load from URL\n",
    "def load_from_url(url):\n",
    "    \"\"\"Load dataset from a public URL\"\"\"\n",
    "    try:\n",
    "        df_url = pd.read_csv(url)\n",
    "        print(f\"✅ Dataset loaded from URL! Shape: {df_url.shape}\")\n",
    "        return df_url\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading from URL: {e}\")\n",
    "        return None\n",
    "\n",
    "# Uncomment and run one of these methods if needed:\n",
    "# df = load_from_drive()\n",
    "# df = load_from_url('https://raw.githubusercontent.com/your-repo/ESP_PDHG_WT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab Package Installation\n",
    "# Run this cell first in Google Colab\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"🌐 Running in Google Colab\")\n",
    "    \n",
    "    # Check what's already available in Colab by default\n",
    "    packages_to_install = []\n",
    "    \n",
    "    try:\n",
    "        import xgboost\n",
    "        print(\"✅ XGBoost already available\")\n",
    "    except ImportError:\n",
    "        packages_to_install.append('xgboost')\n",
    "        \n",
    "    try:\n",
    "        import lightgbm\n",
    "        print(\"✅ LightGBM already available\")\n",
    "    except ImportError:\n",
    "        packages_to_install.append('lightgbm')\n",
    "        \n",
    "    try:\n",
    "        import tensorflow\n",
    "        print(\"✅ TensorFlow already available\")\n",
    "    except ImportError:\n",
    "        packages_to_install.append('tensorflow')\n",
    "    \n",
    "    # Install missing packages\n",
    "    if packages_to_install:\n",
    "        print(f\"🔧 Installing missing packages: {', '.join(packages_to_install)}\")\n",
    "        for package in packages_to_install:\n",
    "            !pip install {package} -q\n",
    "        print(\"✅ Package installation complete!\")\n",
    "    else:\n",
    "        print(\"✅ All required packages are already available!\")\n",
    "        \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"💻 Running in local environment\")\n",
    "    print(\"📝 Note: Ensure you have installed: pandas, numpy, matplotlib, seaborn, scikit-learn, xgboost, lightgbm, tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6d6b8",
   "metadata": {},
   "source": [
    "## 1. Data Exploration\n",
    "We'll explore the dataset to understand distributions, missing values, and relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba49c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa061101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Distribution of target\n",
    "sns.histplot(df['PDP_psi'], kde=True)\n",
    "plt.title('Distribution of Downhole Pressure (PDP_psi)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57eb76c",
   "metadata": {},
   "source": [
    "### Univariate Distributions of Numeric Features\n",
    "Visualize the spread of each continuous parameter to understand ranges and detect potential skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73662771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate histograms\n",
    "num_feats = ['PDHG_Dep_ft','Ql_blpd','Gor_scf_bbl','WCT, %','Choke','Freq_Hz','WHP_psi','WHT_degF']\n",
    "plt.figure(figsize=(16,12))\n",
    "for i, col in enumerate(num_feats,1):\n",
    "    plt.subplot(3,3,i)\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(col)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pairplot for subset to check relationships (downsample if needed)\n",
    "sample_df = df.sample(500, random_state=1)\n",
    "sns.pairplot(sample_df[num_feats + ['PDP_psi']], corner=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c504d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "# Use numerical features for correlation\n",
    "num_cols = ['PDHG_Dep_ft','Ql_blpd','Gor_scf_bbl','WCT, %','Choke','Freq_Hz','WHP_psi','WHT_degF','PDP_psi']\n",
    "corr = df[num_cols].corr()\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d7cf9",
   "metadata": {},
   "source": [
    "## 2. Preprocessing and Feature Engineering\n",
    "- Parse dates\n",
    "- Encode categorical features\n",
    "- Scale numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse date and extract features\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "\n",
    "# Encode Well Name\n",
    "le = LabelEncoder()\n",
    "df['Well Name Encoded'] = le.fit_transform(df['Well Name'])\n",
    "\n",
    "# Select features and target\n",
    "features = ['Well Name Encoded', 'PDHG_Dep_ft', 'Ql_blpd', 'Gor_scf_bbl', 'WCT, %', 'Choke', 'Freq_Hz', 'WHP_psi', 'WHT_degF', 'Year', 'Month', 'Day']\n",
    "X = df[features]\n",
    "y = df['PDP_psi']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439a1f2c",
   "metadata": {},
   "source": [
    "## 3. Modeling\n",
    "We'll train multiple regression models to predict downhole pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c379ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_tr, X_te, y_tr, y_te):\n",
    "    model.fit(X_tr, y_tr)\n",
    "    preds = model.predict(X_te)\n",
    "    mae = mean_absolute_error(y_te, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_te, preds))\n",
    "    r2 = r2_score(y_te, preds)\n",
    "    print(f\"{name} - MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.3f}\")\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e942007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "lr_preds = evaluate_model('Linear Regression', LinearRegression(), X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df72626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_preds = evaluate_model('Random Forest', RandomForestRegressor(random_state=42), X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3941ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "xgb_preds = evaluate_model('XGBoost', xgb_model, X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0918c",
   "metadata": {},
   "source": [
    "### 3.4 Artificial Neural Network (ANN)\n",
    "Train a simple feedforward network with two hidden layers to predict downhole pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ANN model\n",
    "ann = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # output\n",
    "])\n",
    "ann.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train\n",
    "history = ann.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "ann_preds = ann.predict(X_test_scaled).flatten()\n",
    "mae_ann  = mean_absolute_error(y_test, ann_preds)\n",
    "rmse_ann = np.sqrt(mean_squared_error(y_test, ann_preds))\n",
    "r2_ann   = r2_score(y_test, ann_preds)\n",
    "print(f\"ANN - MAE: {mae_ann:.2f}, RMSE: {rmse_ann:.2f}, R2: {r2_ann:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4c8727",
   "metadata": {},
   "source": [
    "### 3.5 Hyperparameter Tuning & Advanced Algorithms\n",
    "We use GridSearchCV for RF and XGBoost, and demonstrate SVR and LightGBM as advanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for Random Forest\n",
    "param_grid_rf = {'n_estimators':[50,100],'max_depth':[None,10,20]}\n",
    "gs_rf = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_rf,\n",
    "                     cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "gs_rf.fit(X_train_scaled, y_train)\n",
    "print('Best RF params:', gs_rf.best_params_)\n",
    "rf_tuned_preds = evaluate_model('RF Tuned', gs_rf.best_estimator_, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "# Grid search for XGBoost\n",
    "param_grid_xgb = {'n_estimators':[50,100],'max_depth':[3,6],'learning_rate':[0.01,0.1]}\n",
    "gs_xgb = GridSearchCV(xgb.XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "                      param_grid_xgb, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "gs_xgb.fit(X_train_scaled, y_train)\n",
    "print('Best XGB params:', gs_xgb.best_params_)\n",
    "xgb_tuned_preds = evaluate_model('XGB Tuned', gs_xgb.best_estimator_, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "# Support Vector Regressor\n",
    "svr_preds = evaluate_model('SVR', SVR(kernel='rbf'), X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "# LightGBM Regressor\n",
    "lgb_preds = evaluate_model('LightGBM', lgb.LGBMRegressor(random_state=42), X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c1011e",
   "metadata": {},
   "source": [
    "### 3.6 Additional Feature Engineering Ideas\n",
    "\n",
    "For enhanced model performance, consider these additional features:\n",
    "\n",
    "- **Pressure gradient**: Difference between WHP_psi and PDP_psi\n",
    "- **Rate-of-change features**: Flow and pressure derivatives over time  \n",
    "- **ESP power data**: Motor current and voltage measurements\n",
    "- **Fluid properties**: Oil density, viscosity, API gravity\n",
    "- **Reservoir parameters**: Porosity, permeability, skin factor\n",
    "- **Well intervention**: Time since last maintenance or workover\n",
    "- **Seasonal effects**: Cyclical patterns in production\n",
    "- **Well age**: Days/months since well completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59460877",
   "metadata": {},
   "source": [
    "## 3.7 Model Performance Summary & Feature Importance\n",
    "\n",
    "Let's create a comprehensive comparison table and analyze which features are most important for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0570a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison table\n",
    "def create_performance_table():\n",
    "    models_dict = {\n",
    "        'Linear Regression': lr_preds,\n",
    "        'Random Forest': rf_preds,\n",
    "        'XGBoost': xgb_preds,\n",
    "        'RF Tuned': rf_tuned_preds,\n",
    "        'XGB Tuned': xgb_tuned_preds,\n",
    "        'SVR': svr_preds,\n",
    "        'LightGBM': lgb_preds,\n",
    "        'ANN': ann_preds\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    for name, preds in models_dict.items():\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        r2 = r2_score(y_test, preds)\n",
    "        results.append({'Model': name, 'MAE': mae, 'RMSE': rmse, 'R²': r2})\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = results_df.sort_values('R²', ascending=False)\n",
    "    return results_df\n",
    "\n",
    "# Display performance table\n",
    "perf_table = create_performance_table()\n",
    "print(\"🏆 Model Performance Ranking (by R²):\")\n",
    "print(\"=\" * 50)\n",
    "display(perf_table.round(3))\n",
    "\n",
    "# Feature importance for tree-based models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Feature Importance Analysis', fontsize=16)\n",
    "\n",
    "# Random Forest\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': gs_rf.best_estimator_.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "axes[0,0].barh(rf_importance['feature'], rf_importance['importance'])\n",
    "axes[0,0].set_title('Random Forest Feature Importance')\n",
    "axes[0,0].set_xlabel('Importance')\n",
    "\n",
    "# XGBoost\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': gs_xgb.best_estimator_.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "axes[0,1].barh(xgb_importance['feature'], xgb_importance['importance'])\n",
    "axes[0,1].set_title('XGBoost Feature Importance')\n",
    "axes[0,1].set_xlabel('Importance')\n",
    "\n",
    "# LightGBM\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "lgb_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'importance': lgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "axes[1,0].barh(lgb_importance['feature'], lgb_importance['importance'])\n",
    "axes[1,0].set_title('LightGBM Feature Importance')\n",
    "axes[1,0].set_xlabel('Importance')\n",
    "\n",
    "# Combined importance (average)\n",
    "combined_importance = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'rf_imp': gs_rf.best_estimator_.feature_importances_,\n",
    "    'xgb_imp': gs_xgb.best_estimator_.feature_importances_,\n",
    "    'lgb_imp': lgb_model.feature_importances_\n",
    "})\n",
    "combined_importance['avg_importance'] = combined_importance[['rf_imp', 'xgb_imp', 'lgb_imp']].mean(axis=1)\n",
    "combined_importance = combined_importance.sort_values('avg_importance', ascending=True)\n",
    "\n",
    "axes[1,1].barh(combined_importance['feature'], combined_importance['avg_importance'])\n",
    "axes[1,1].set_title('Average Feature Importance')\n",
    "axes[1,1].set_xlabel('Average Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"🔍 Top 5 Most Important Features:\")\n",
    "top_features = combined_importance.nlargest(5, 'avg_importance')[['feature', 'avg_importance']]\n",
    "for idx, row in top_features.iterrows():\n",
    "    print(f\"  {row['feature']}: {row['avg_importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f54085",
   "metadata": {},
   "source": [
    "## 4. Results and Visualization\n",
    "Compare actual vs predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Actual vs Predicted for all models\n",
    "plt.figure(figsize=(16,12))\n",
    "models_preds = {\n",
    "    'Linear Regression': lr_preds,\n",
    "    'Random Forest': rf_preds,\n",
    "    'XGBoost': xgb_preds,\n",
    "    'RF Tuned': rf_tuned_preds,\n",
    "    'XGB Tuned': xgb_tuned_preds,\n",
    "    'SVR': svr_preds,\n",
    "    'LightGBM': lgb_preds,\n",
    "    'ANN': ann_preds\n",
    "}\n",
    "\n",
    "for i, (name, preds) in enumerate(models_preds.items(), 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    plt.scatter(y_test, preds, alpha=0.6)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    plt.title(f'{name}')\n",
    "    plt.xlabel('Actual PDP_psi')\n",
    "    plt.ylabel('Predicted PDP_psi')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.2 ANN Training History\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('ANN Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Val MAE')\n",
    "plt.title('ANN Training MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4.3 Model Serialization for Production\n",
    "print(\"💾 Saving trained models...\")\n",
    "joblib.dump(gs_xgb.best_estimator_, 'best_xgb_model.pkl')\n",
    "joblib.dump(scaler, 'feature_scaler.pkl')\n",
    "ann.save('ann_model.h5')\n",
    "\n",
    "print('✅ Models saved successfully:')\n",
    "print('  📁 best_xgb_model.pkl (XGBoost)')\n",
    "print('  📁 feature_scaler.pkl (StandardScaler)')  \n",
    "print('  📁 ann_model.h5 (Neural Network)')\n",
    "\n",
    "# Download models in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import files\n",
    "    import os\n",
    "    \n",
    "    print('\\n📥 Preparing models for download...')\n",
    "    \n",
    "    # Check if files exist before downloading\n",
    "    model_files = ['best_xgb_model.pkl', 'feature_scaler.pkl', 'ann_model.h5']\n",
    "    for file in model_files:\n",
    "        if os.path.exists(file):\n",
    "            print(f'📦 Downloading {file}...')\n",
    "            files.download(file)\n",
    "        else:\n",
    "            print(f'⚠️ {file} not found')\n",
    "    \n",
    "    print('✅ Download complete! Check your Downloads folder.')\n",
    "    \n",
    "except ImportError:\n",
    "    print('💻 Running locally - models saved to current directory')\n",
    "except Exception as e:\n",
    "    print(f'❌ Download error: {e}')\n",
    "    print('💡 You can manually download files from the Colab file browser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d93116d",
   "metadata": {},
   "source": [
    "### 4.4 Model Diagnostics & Residual Analysis\n",
    "Analyze model performance patterns and identify potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59bc0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Analysis for Best Models (XGB Tuned and ANN)\n",
    "best_models = {\n",
    "    'XGB Tuned': xgb_tuned_preds,\n",
    "    'ANN': ann_preds\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Model Diagnostics: Residual Analysis', fontsize=16)\n",
    "\n",
    "for i, (name, preds) in enumerate(best_models.items()):\n",
    "    residuals = y_test - preds\n",
    "    \n",
    "    # Residual vs Predicted\n",
    "    axes[i, 0].scatter(preds, residuals, alpha=0.6)\n",
    "    axes[i, 0].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[i, 0].set_xlabel('Predicted Values')\n",
    "    axes[i, 0].set_ylabel('Residuals')\n",
    "    axes[i, 0].set_title(f'{name}: Residuals vs Predicted')\n",
    "    \n",
    "    # Residual Distribution\n",
    "    axes[i, 1].hist(residuals, bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[i, 1].set_xlabel('Residuals')\n",
    "    axes[i, 1].set_ylabel('Frequency')\n",
    "    axes[i, 1].set_title(f'{name}: Residual Distribution')\n",
    "    axes[i, 1].axvline(x=0, color='red', linestyle='--')\n",
    "    \n",
    "    # Q-Q Plot\n",
    "    from scipy import stats\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[i, 2])\n",
    "    axes[i, 2].set_title(f'{name}: Q-Q Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error analysis by well\n",
    "print(\"📊 Error Analysis by Well:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Add well information to test set for analysis\n",
    "test_indices = X_test.index\n",
    "wells_test = df.loc[test_indices, 'Well Name'].values\n",
    "xgb_residuals = y_test - xgb_tuned_preds\n",
    "\n",
    "error_by_well = pd.DataFrame({\n",
    "    'Well': wells_test,\n",
    "    'Actual': y_test.values,\n",
    "    'Predicted': xgb_tuned_preds,\n",
    "    'Residual': xgb_residuals,\n",
    "    'Abs_Error': np.abs(xgb_residuals)\n",
    "})\n",
    "\n",
    "well_stats = error_by_well.groupby('Well').agg({\n",
    "    'Abs_Error': ['mean', 'std', 'count'],\n",
    "    'Residual': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "print(well_stats)\n",
    "\n",
    "# Prediction intervals (for demonstration with XGBoost)\n",
    "print(f\"\\n🎯 XGBoost Model Performance Summary:\")\n",
    "print(f\"Mean Absolute Error: {np.mean(np.abs(xgb_residuals)):.2f} psi\")\n",
    "print(f\"Standard Deviation: {np.std(xgb_residuals):.2f} psi\")\n",
    "print(f\"95% Prediction Interval: ± {1.96 * np.std(xgb_residuals):.2f} psi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368b8f4",
   "metadata": {},
   "source": [
    "## 5. Production Deployment Notes\n",
    "\n",
    "### Model Serialization & Loading\n",
    "```python\n",
    "# Loading saved models for production\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load models\n",
    "xgb_model = joblib.load('best_xgb_model.pkl')\n",
    "scaler = joblib.load('feature_scaler.pkl')\n",
    "ann_model = load_model('ann_model.h5')\n",
    "\n",
    "# Make predictions on new data\n",
    "def predict_pressure(new_data):\n",
    "    scaled_data = scaler.transform(new_data)\n",
    "    xgb_pred = xgb_model.predict(scaled_data)\n",
    "    ann_pred = ann_model.predict(scaled_data)\n",
    "    return xgb_pred, ann_pred\n",
    "```\n",
    "\n",
    "### API Deployment Options\n",
    "- **Flask/FastAPI**: Create REST API endpoints\n",
    "- **Docker**: Containerize for cloud deployment  \n",
    "- **AWS SageMaker**: Managed ML inference\n",
    "- **Azure ML**: Enterprise ML platform\n",
    "- **Real-time monitoring**: Track model drift and performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deee98a",
   "metadata": {},
   "source": [
    "## 6. Exercises & Next Steps\n",
    "\n",
    "### 🎓 Student Exercises (Choose 2-3 based on your interest)\n",
    "\n",
    "#### **Beginner Level:**\n",
    "1. **Feature Engineering**: Implement pressure gradient (WHP_psi - PDP_psi) as a new feature\n",
    "2. **Data Visualization**: Create box plots showing PDP_psi distribution by well\n",
    "3. **Model Comparison**: Add Decision Tree Regressor and compare performance\n",
    "\n",
    "#### **Intermediate Level:**\n",
    "4. **Cross-Validation**: Implement TimeSeriesSplit for proper temporal validation\n",
    "5. **Ensemble Methods**: Create a VotingRegressor combining top 3 models\n",
    "6. **Hyperparameter Optimization**: Use RandomizedSearchCV on more parameters\n",
    "7. **Well-Specific Analysis**: Train separate models for each well and compare\n",
    "\n",
    "#### **Advanced Level:**\n",
    "8. **Custom Features**: Create rolling averages for pressure and flow rate\n",
    "9. **Anomaly Detection**: Identify unusual pressure readings using IsolationForest\n",
    "10. **Production API**: Build a Flask/FastAPI endpoint for real-time predictions\n",
    "11. **Model Monitoring**: Implement drift detection using Evidently AI\n",
    "\n",
    "### 🚀 Advanced Research Topics\n",
    "\n",
    "#### **Physics-Informed Machine Learning:**\n",
    "- Incorporate ESP performance curves and fluid dynamics\n",
    "- Add thermodynamic constraints to neural networks\n",
    "- Use physics-based loss functions\n",
    "\n",
    "#### **Time Series & Sequential Modeling:**\n",
    "- LSTM networks for temporal pressure prediction\n",
    "- Prophet for seasonal trend analysis\n",
    "- Kalman filters for real-time state estimation\n",
    "\n",
    "#### **Uncertainty Quantification:**\n",
    "- Bayesian neural networks for prediction intervals\n",
    "- Monte Carlo dropout for uncertainty estimation\n",
    "- Conformal prediction for reliable intervals\n",
    "\n",
    "#### **Transfer Learning & Domain Adaptation:**\n",
    "- Apply models trained on one field to another\n",
    "- Few-shot learning for new wells\n",
    "- Domain adversarial training for robust models\n",
    "\n",
    "### 🏭 Real-World Implementation Challenges\n",
    "\n",
    "#### **Data Quality & Engineering:**\n",
    "- Handle sensor failures and missing data\n",
    "- Real-time data streaming and processing\n",
    "- Data validation and quality checks\n",
    "\n",
    "#### **Production Deployment:**\n",
    "- Model versioning and A/B testing\n",
    "- Monitoring model performance and drift\n",
    "- Automated retraining pipelines\n",
    "- Edge computing for downhole sensors\n",
    "\n",
    "#### **Business Integration:**\n",
    "- Cost-benefit analysis of predictions\n",
    "- Integration with existing SCADA systems\n",
    "- Regulatory compliance and documentation\n",
    "- Safety-critical system requirements\n",
    "\n",
    "#### **Maintenance & Operations:**\n",
    "- Model interpretability for domain experts\n",
    "- Alert systems for abnormal conditions\n",
    "- Predictive maintenance scheduling\n",
    "- Human-in-the-loop validation\n",
    "\n",
    "### 📚 Recommended Reading\n",
    "- \"Hands-On Machine Learning\" by Aurélien Géron\n",
    "- \"Pattern Recognition and Machine Learning\" by Christopher Bishop\n",
    "- \"The Elements of Statistical Learning\" by Hastie, Tibshirani, Friedman\n",
    "- \"Applied Predictive Modeling\" by Kuhn & Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b351077",
   "metadata": {},
   "source": [
    "## 7. Bonus: Quick Implementation Examples\n",
    "\n",
    "### 🎯 Ready-to-Use Code Snippets for Common Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9df677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS 1: Create Ensemble Model\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Create ensemble of top 3 models\n",
    "ensemble = VotingRegressor([\n",
    "    ('xgb', gs_xgb.best_estimator_),\n",
    "    ('lgb', lgb.LGBMRegressor(random_state=42)),\n",
    "    ('ann', ann)  # Note: sklearn wrapper needed for ANN\n",
    "])\n",
    "\n",
    "# For demo purposes (ANN needs wrapper for sklearn ensemble)\n",
    "print(\"💡 Ensemble model structure created\")\n",
    "print(\"   - XGBoost (tuned)\")\n",
    "print(\"   - LightGBM\") \n",
    "print(\"   - Neural Network\")\n",
    "\n",
    "# BONUS 2: Simple Time Series Cross-Validation\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "cv_scores = []\n",
    "\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    # Scale data\n",
    "    scaler_cv = StandardScaler()\n",
    "    X_tr_scaled = scaler_cv.fit_transform(X_tr)\n",
    "    X_val_scaled = scaler_cv.transform(X_val)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    model_cv = xgb.XGBRegressor(random_state=42)\n",
    "    model_cv.fit(X_tr_scaled, y_tr)\n",
    "    pred_cv = model_cv.predict(X_val_scaled)\n",
    "    score_cv = r2_score(y_val, pred_cv)\n",
    "    cv_scores.append(score_cv)\n",
    "\n",
    "print(f\"\\n⏰ Time Series CV Results (XGBoost):\")\n",
    "print(f\"   CV Scores: {[f'{s:.3f}' for s in cv_scores]}\")\n",
    "print(f\"   Mean R²: {np.mean(cv_scores):.3f} ± {np.std(cv_scores):.3f}\")\n",
    "\n",
    "# BONUS 3: Feature Engineering Example\n",
    "def create_engineered_features(df):\n",
    "    \"\"\"Create additional features for enhanced prediction\"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # Pressure gradient\n",
    "    df_eng['Pressure_Gradient'] = df_eng['WHP_psi'] - df_eng['PDP_psi']\n",
    "    \n",
    "    # Production intensity\n",
    "    df_eng['Production_Intensity'] = df_eng['Ql_blpd'] / df_eng['PDHG_Dep_ft']\n",
    "    \n",
    "    # Power proxy (frequency * pressure)\n",
    "    df_eng['Power_Proxy'] = df_eng['Freq_Hz'] * df_eng['WHP_psi']\n",
    "    \n",
    "    # Water cut ratio\n",
    "    df_eng['WCT_Ratio'] = df_eng['WCT, %'] / (100 - df_eng['WCT, %'] + 1e-6)\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "print(f\"\\n🔧 Feature Engineering Function Created:\")\n",
    "print(\"   - Pressure_Gradient = WHP_psi - PDP_psi\")\n",
    "print(\"   - Production_Intensity = Ql_blpd / PDHG_Dep_ft\")\n",
    "print(\"   - Power_Proxy = Freq_Hz * WHP_psi\")\n",
    "print(\"   - WCT_Ratio = WCT / (100 - WCT)\")\n",
    "\n",
    "# BONUS 4: Simple API Prediction Function\n",
    "def predict_downhole_pressure(well_data, model_path='best_xgb_model.pkl', scaler_path='feature_scaler.pkl'):\n",
    "    \"\"\"\n",
    "    Predict downhole pressure for new well data\n",
    "    \n",
    "    Args:\n",
    "        well_data: dict with keys matching feature names\n",
    "        model_path: path to saved model\n",
    "        scaler_path: path to saved scaler\n",
    "    \n",
    "    Returns:\n",
    "        predicted pressure in psi\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load model and scaler\n",
    "        model = joblib.load(model_path)\n",
    "        scaler = joblib.load(scaler_path)\n",
    "        \n",
    "        # Prepare data\n",
    "        input_array = np.array([list(well_data.values())]).reshape(1, -1)\n",
    "        input_scaled = scaler.transform(input_array)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = model.predict(input_scaled)[0]\n",
    "        \n",
    "        return {\n",
    "            'predicted_pressure_psi': round(prediction, 2),\n",
    "            'status': 'success'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'error': str(e),\n",
    "            'status': 'failed'\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "example_data = {\n",
    "    'Well Name Encoded': 1,\n",
    "    'PDHG_Dep_ft': 8000,\n",
    "    'Ql_blpd': 5000,\n",
    "    'Gor_scf_bbl': 100,\n",
    "    'WCT, %': 15,\n",
    "    'Choke': 35,\n",
    "    'Freq_Hz': 55,\n",
    "    'WHP_psi': 800,\n",
    "    'WHT_degF': 140,\n",
    "    'Year': 2025,\n",
    "    'Month': 8,\n",
    "    'Day': 24\n",
    "}\n",
    "\n",
    "print(f\"\\n🎯 API Prediction Function Created:\")\n",
    "print(\"   Usage: predict_downhole_pressure(well_data)\")\n",
    "print(\"   Returns: {'predicted_pressure_psi': value, 'status': 'success'}\")\n",
    "\n",
    "print(f\"\\n✨ Bonus implementations completed!\")\n",
    "print(\"   🔗 Ensemble modeling approach\")\n",
    "print(\"   ⏰ Time series cross-validation\")  \n",
    "print(\"   🔧 Feature engineering pipeline\")\n",
    "print(\"   🌐 Production API function\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
