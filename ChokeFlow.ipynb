{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SPE-PFAC01/ALCE/blob/main/ChokeFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98b7d1e1",
      "metadata": {
        "id": "98b7d1e1"
      },
      "source": [
        "# Virtual Flow Meter using critical flow data at production choke\n",
        "### Data and methodologies mentioned in reference below are reproduced here.\n",
        "##### Reference: Barjouei, H.S., Ghorbani, H., Mohamadian, N. et al. Prediction performance advantages of deep machine learning algorithms for two-phase flow rates through wellhead chokes. J Petrol Explor Prod Technol 11, 1233–1261 (2021). https://link.springer.com/article/10.1007/s13202-021-01087-4 [Accessed 3 Jun 2021]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "akHoH99h4h0r"
      },
      "id": "akHoH99h4h0r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d94bdc74",
      "metadata": {
        "id": "d94bdc74"
      },
      "source": [
        "### Load Python Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "785d91ac",
      "metadata": {
        "id": "785d91ac"
      },
      "outputs": [],
      "source": [
        "# Data storage, exploration\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# The following three lines allow multiple and non-truncated outputs\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84e68d7",
      "metadata": {
        "id": "a84e68d7"
      },
      "source": [
        "# Import Data & Preliminary Data Exploration\n",
        "1. How many data records?\n",
        "2. How many variables / features or columns in each data record?\n",
        "3. Peek at the first five records and the last five records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "003abe04",
      "metadata": {
        "id": "003abe04"
      },
      "outputs": [],
      "source": [
        "# Import Data\n",
        "vfm = pd.read_csv('/content/drive/MyDrive/ALCE/SorushDatasetChokeFlow.csv')\n",
        "vfm.columns = ['SampleID', 'Well', 'D64', 'Pwh', 'γo', 'GLR', 'QL']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b19d75b1",
      "metadata": {
        "id": "b19d75b1"
      },
      "outputs": [],
      "source": [
        "vfm.info()\n",
        "vfm.head()\n",
        "vfm.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a55ffb0e",
      "metadata": {
        "id": "a55ffb0e"
      },
      "outputs": [],
      "source": [
        "# Plot Statistics\n",
        "plt.figure(figsize=(15,6));\n",
        "plt.yscale(\"log\");\n",
        "plt.grid('y');\n",
        "#_ = plt.xticks(rotation='vertical');\n",
        "sns.boxplot(data=vfm.drop(columns=['SampleID', 'Well']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e178a7d1",
      "metadata": {
        "id": "e178a7d1"
      },
      "source": [
        "### Boxplot Visualization\n",
        "1. In the visualization above, why some boxes are very tall (long color bars).\n",
        "2. Which variable has the smallest distribution?\n",
        "3. Which variable is widely distributed?\n",
        "4. What does it mean when one whisker is longer than the other?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "593d23ac",
      "metadata": {
        "id": "593d23ac"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,6));\n",
        "plt.grid('y');\n",
        "sns.boxplot(data=vfm, x=\"Well\", y=\"QL\");\n",
        "plt.figure(figsize=(15,6));\n",
        "plt.grid('y');\n",
        "sns.boxplot(data=vfm, x=\"Well\", y=\"GLR\");"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c39d91b3",
      "metadata": {
        "id": "c39d91b3"
      },
      "source": [
        "### Histograms\n",
        "Help visualize how measurements are distributed.\n",
        "Wouldn't we like them to be normally distributed?!?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36c687d3",
      "metadata": {
        "id": "36c687d3"
      },
      "outputs": [],
      "source": [
        "vfm.drop(columns=['SampleID']).hist(figsize=(20, 20));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cccb8cb1",
      "metadata": {
        "id": "cccb8cb1"
      },
      "outputs": [],
      "source": [
        "plot_vars = ['D64', 'Pwh', 'γo', 'GLR', 'QL']\n",
        "# Define a function to plot histogram and scatterplot for the specified variables/columns of provided dataframe\n",
        "def plotPairgrid(df, plot_vars):\n",
        "    g = sns.PairGrid(data=df, vars=plot_vars, diag_sharey=False);\n",
        "    g.map_upper(sns.scatterplot, s=15);\n",
        "    g.map_lower(sns.kdeplot, fill='True');\n",
        "    g.map_diag(sns.kdeplot, lw=2);\n",
        "\n",
        "plotPairgrid(vfm, plot_vars);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e108786a",
      "metadata": {
        "id": "e108786a"
      },
      "source": [
        "### Correlations between Variables\n",
        "Let's find out if any of the two variables are correlated by calculating correlation coefficients between them.\n",
        "1. Positive value (positive correlation) means one increases with another in the dataset; and\n",
        "2. Negative value (negative correlation) means one decreases while another increases and vice versa.\n",
        "3. Magnitude of the correlation coefficient indicates strength of the correlation.\n",
        "\n",
        "There are multiple ways to perform this task. We will calculate Pearson and Spearman coefficeints.\n",
        "#### Pearson Correlation Coefficient\n",
        "Pearson correlation assumes that the data we are comparing is normally distributed. When that assumption is not true, the correlation value is reflecting the true association.\n",
        "\n",
        "#### Spearman Rank Correlation\n",
        "Spearman correlation does not assume that data is from a specific distribution, so it is a non-parametric correlation measure. Spearman correlation is also known as Spearman’s rank correlation as it computes correlation coefficient on rank values of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82f4a271",
      "metadata": {
        "id": "82f4a271"
      },
      "outputs": [],
      "source": [
        "def plot_corrcoeff(method):\n",
        "    dfCorr = vfm[plot_vars].corr(method=method)\n",
        "    g1 = sns.heatmap(dfCorr, center=0.0, linewidths=0.1, square=True, annot=True, vmin=-1, vmax=1., fmt='1.2f')\n",
        "    g1.set_xticklabels(g1.get_xticklabels(), rotation=30);\n",
        "    g1.set_title(method.capitalize() + ' Correlation Coefficients - Heatmap')\n",
        "    plt.show()\n",
        "\n",
        "# Pearson Correlation Coefficient\n",
        "plot_corrcoeff(method='pearson')\n",
        "\n",
        "# Spearman Rank Correlation\n",
        "plot_corrcoeff(method='spearman')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c2d734",
      "metadata": {
        "id": "e0c2d734"
      },
      "source": [
        "Spearman’s correlation coefficients shown above reveal that the input variables,\n",
        "* GLR and γo are inversely related to QL,\n",
        "* Pwh display +ve correlation with QL and -ve correlation with D64.\n",
        "* D64 shows the weakest correlation with QL of the four input variables evaluated, i.e., the flowrate is insensitive to choke size. The prevailing flow through the wellhead chokes of the Sorush oil field conforms to a critical flow regime"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66372f08",
      "metadata": {
        "id": "66372f08"
      },
      "source": [
        "# Data Exploration\n",
        "### Missing Data at Macro Level\n",
        "Are there any null (NaN) measurements for numeric data columns?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cad4e6b",
      "metadata": {
        "id": "5cad4e6b"
      },
      "outputs": [],
      "source": [
        "# Q1. How many nulls are there?\n",
        "vfm.isnull().sum()\n",
        "\n",
        "# Q2. How many values are zero or -ve\n",
        "(vfm[plot_vars] <= 0.0).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e34a5aa2",
      "metadata": {
        "id": "e34a5aa2"
      },
      "source": [
        "### Prepare features and target data arrays\n",
        "1. Convert from data-frame column(s) to numpy-arrays\n",
        "2. Normalize all the features (why?)\n",
        "3. Separate the data in training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b05f9ae0",
      "metadata": {
        "id": "b05f9ae0"
      },
      "outputs": [],
      "source": [
        "# Separate targets from inputs\n",
        "X = vfm[plot_vars].drop(columns='QL').to_numpy()\n",
        "y = vfm['QL'].to_numpy()\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a777f9ae",
      "metadata": {
        "id": "a777f9ae"
      },
      "outputs": [],
      "source": [
        "# Do we need to normalize X - input features?\n",
        "X[:5]\n",
        "# Normalize all the samples between [-1, +1]\n",
        "X = (X - X.min(axis=0))/(X.max(axis=0) - X.min(axis=0))*2 - 1\n",
        "X[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd690ea3",
      "metadata": {
        "id": "cd690ea3"
      },
      "outputs": [],
      "source": [
        "## Sub-divide datatest into training and testing: 80 - 20% split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2, random_state=1002)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa46ca16",
      "metadata": {
        "id": "aa46ca16"
      },
      "outputs": [],
      "source": [
        "# Set up a callable function for relevant metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
        "\n",
        "def calc_predMetrics(y_true, y_pred, method_name):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {'method':method_name, 'rmse':rmse, 'MAE': mae, 'MAPE':mape, 'R2':r2}\n",
        "\n",
        "# Initialize list to store calculated metric values for each model\n",
        "pred_perf_metric = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb4ff90b",
      "metadata": {
        "id": "bb4ff90b"
      },
      "source": [
        "### Support Vector Regression (SVR)\n",
        "In this study, the RBF kernel is used with the SVR algorithm to predict two-phase flow rate (Ql) through a wellhead choke."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28558c29",
      "metadata": {
        "id": "28558c29"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "svr_rbf = SVR(kernel=\"rbf\", C=100000, gamma=0.05, epsilon=0.1)\n",
        "y_pred_svr = svr_rbf.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "pred_perf_metric.append(\n",
        "    calc_predMetrics(y_test, y_pred_svr, method_name=\"SVR\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e249efbf",
      "metadata": {
        "id": "e249efbf"
      },
      "source": [
        "### Decision Tree (DT)\n",
        "In this study, a the scikit learn (sklearn) decision tree module is coded applying the “gini” criterion to establish feature importance and the “best” splitter is applied to decide which feature and the value of the threshold to apply in making each split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53dfefcf",
      "metadata": {
        "id": "53dfefcf"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "dt = DecisionTreeRegressor(max_depth=100, splitter='best',\n",
        "                           criterion='squared_error', random_state=1002)\n",
        "y_pred_dt = dt.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "pred_perf_metric.append(\n",
        "    calc_predMetrics(y_test, y_pred_dt, method_name=\"DT\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2692c25e",
      "metadata": {
        "id": "2692c25e"
      },
      "source": [
        "### RandomForest (RF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "958b87da",
      "metadata": {
        "id": "958b87da"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf = RandomForestRegressor(n_estimators=1000, max_depth=1000,\n",
        "                           criterion='squared_error', random_state=0)\n",
        "y_pred_rf = rf.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "pred_perf_metric.append(\n",
        "    calc_predMetrics(y_test, y_pred_rf, method_name=\"Random Forest\" ))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "968e044d",
      "metadata": {
        "id": "968e044d"
      },
      "source": [
        "### ExtraTrees (ET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cd9558c",
      "metadata": {
        "id": "8cd9558c"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "et = ExtraTreesRegressor(n_estimators=1000, max_depth=1000,\n",
        "                           criterion='squared_error', random_state=0)\n",
        "y_pred_et = et.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "pred_perf_metric.append(\n",
        "    calc_predMetrics(y_test, y_pred_et, method_name=\"Extra Trees\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b97ea02",
      "metadata": {
        "id": "1b97ea02"
      },
      "source": [
        "### eXtreme Gradient Boosting (XGB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f6e5a2b",
      "metadata": {
        "id": "4f6e5a2b"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "xgbm = xgb.XGBRegressor()\n",
        "y_pred_xgb = xgbm.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "pred_perf_metric.append(\n",
        "    calc_predMetrics(y_test, y_pred_xgb, method_name=\"XGBoost\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "930bd317",
      "metadata": {
        "id": "930bd317"
      },
      "source": [
        "### Artificial Neural Network (ANN) using Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2105ffa8",
      "metadata": {
        "id": "2105ffa8"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "callback = EarlyStopping(monitor='loss', min_delta=0.001, patience=25)\n",
        "\n",
        "# build model\n",
        "nn = Sequential()\n",
        "nn.add(Dense(50, input_dim=4, activation='selu', kernel_initializer='he_uniform'))\n",
        "nn.add(Dense(50, activation='selu', kernel_initializer='he_uniform'))\n",
        "nn.add(Dense(50, activation='selu', kernel_initializer='he_uniform'))\n",
        "nn.add(Dense(50, activation='selu', kernel_initializer='he_uniform'))\n",
        "nn.add(Dense(50, activation='selu', kernel_initializer='he_uniform'))\n",
        "nn.add(Dense(50, activation='selu', kernel_initializer='he_uniform'))\n",
        "nn.add(Dense(1, activation='selu'))\n",
        "\n",
        "# compile model\n",
        "nn.compile(loss='mean_squared_error', optimizer=RMSprop(learning_rate=0.01))\n",
        "# fit model\n",
        "nn.fit(X_train, y_train, epochs=100, verbose=0)\n",
        "\n",
        "# evaluate the model\n",
        "y_pred_nn = nn.predict(X_test)\n",
        "\n",
        "pred_perf_metric.append(\n",
        "    calc_predMetrics(y_test, y_pred_nn, method_name=\"ANN\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a66acdf7",
      "metadata": {
        "id": "a66acdf7"
      },
      "source": [
        "### Plot relative performance of methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72d615f0",
      "metadata": {
        "id": "72d615f0"
      },
      "outputs": [],
      "source": [
        "# Convert predicted performance metric list of dictionaries into a dataframe\n",
        "plotDf = pd.DataFrame(pred_perf_metric)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(12, 6), sharey=True)\n",
        "# Plot # 1 - Root Mean Square Error\n",
        "rect1 = axs[0].barh(plotDf.method, plotDf.rmse);\n",
        "axs[0].set_title('RMSE, bbls/day');\n",
        "axs[0].bar_label(rect1, padding=1, fmt='%.1f');\n",
        "axs[0].set_xlim(0., 4000.);\n",
        "\n",
        "# Plot # 2 - Mean Absolute Percentage Error (MAPE)\n",
        "rect2 = axs[1].barh(plotDf.method, plotDf.MAPE);\n",
        "axs[1].set_title('MAPE, %');\n",
        "axs[1].bar_label(rect2, padding=1, fmt='%.3f');\n",
        "axs[1].set_xlim(0., 0.6);\n",
        "\n",
        "# Plot # 3 - R2 Coefficient of Determination\n",
        "rect3 = axs[2].barh(plotDf.method, plotDf.R2);\n",
        "axs[2].set_title('R2 Coefficient');\n",
        "axs[2].bar_label(rect3, padding=0, fmt='%.3f');\n",
        "axs[2].set_xlim(0., 1.2);\n",
        "\n",
        "fig.suptitle('ML Methods Comparison')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}